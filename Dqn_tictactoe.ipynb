{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbf6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input layer size: 3x3x2 (18)\n",
    "        # Hidden layers: 27x27x27\n",
    "        # Output layer: 9\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(64, 9),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, state = None, info={}):\n",
    "        assert obs.shape[-1] == 9\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch,-1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f089d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09f13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs, player = 'player_1'):\n",
    "    state = obs['observation'][:,:,0] - obs['observation'][:,:,1]\n",
    "    if player == 'player_1':\n",
    "        return state.reshape(1,-1)\n",
    "    else:\n",
    "        return -state.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4602f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity=10e5):\n",
    "        self.capacity = int(capacity)\n",
    "        self.memory = deque([], maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        assert batch_size <= self.capacity, \"Batch size larger than capacity\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def initialize(self, env, player = 'player_1'):\n",
    "        # Fill with random transitions\n",
    "        players = env.agents\n",
    "        initial_state = np.ones(9)*-1\n",
    "        previous_state = initial_state\n",
    "        previous_action = -1\n",
    "        previous_reward = 0\n",
    "        while len(self.memory) < self.capacity:\n",
    "            env.reset()\n",
    "            obs, reward, done, truncation, info = env.last()\n",
    "            while done == False:\n",
    "                p = env.agent_selection\n",
    "                if p != player:\n",
    "                    a = random.sample(range(9), 1)[0]\n",
    "                    env.step(a)\n",
    "                    obs, reward, done, truncation, info = env.last()\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                state = get_state(obs, p)\n",
    "                if previous_action != -1:\n",
    "                    self.push(previous_state, previous_action, state, r)\n",
    "                \n",
    "                a = random.sample(range(9), 1)[0]\n",
    "                if obs['action_mask'][a] == 1:\n",
    "                    env.step(a)\n",
    "                    obs, reward, done, truncation, info = env.last()\n",
    "                    r = env.rewards[p]\n",
    "                else:\n",
    "                    done = True\n",
    "                    self.push(state, a, np.ones(state.shape), -10)\n",
    "                    \n",
    "                previous_state = state\n",
    "                previous_action = a\n",
    "                previous_reward = r\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605e4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "episodes = 500000\n",
    "batch_size = 8\n",
    "eps_start = 1\n",
    "eps_end = 0.1\n",
    "rate = 1/100000 * np.log(eps_end/eps_start)\n",
    "capacity = 200000\n",
    "lr = 0.00001\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "training_freq = 1\n",
    "target_update_freq = 100\n",
    "eval_episodes = 2000\n",
    "eval_freq = 2000\n",
    "save_freq = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265cfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "obs, reward, termination, truncation, info = env.last()\n",
    "state_shape = obs['observation'].shape\n",
    "action_shape = obs['action_mask'].shape\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "X_p_net = Net()\n",
    "X_p_net.to(device)\n",
    "O_p_net = Net()\n",
    "O_p_net.to(device)\n",
    "policy_nets = {'player_1': X_p_net, 'player_2': O_p_net}\n",
    "X_target = Net()\n",
    "X_target.to(device)\n",
    "X_target.load_state_dict(X_p_net.state_dict())\n",
    "O_target = Net()\n",
    "O_target.to(device)\n",
    "O_target.load_state_dict(O_p_net.state_dict())\n",
    "target_nets = {'player_1': X_target, 'player_2': O_target}\n",
    "\n",
    "X_memory = ReplayMemory(capacity)\n",
    "O_memory = ReplayMemory(capacity)\n",
    "\n",
    "memories = {'player_1': X_memory, 'player_2': O_memory}\n",
    "\n",
    "optimizer_X = torch.optim.AdamW(policy_nets['player_1'].parameters(), lr=lr, amsgrad=True)\n",
    "optimizer_O = torch.optim.AdamW(policy_nets['player_2'].parameters(), lr=lr, amsgrad=True)\n",
    "\n",
    "optimizers = {'player_1': optimizer_X, 'player_2': optimizer_O}\n",
    "\n",
    "criterion_1 = nn.MSELoss()\n",
    "criterion_2 = nn.MSELoss()\n",
    "crits = {'player_1':criterion_1, 'player_2': criterion_2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b5b55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, n, policy, player = 'player_1'):\n",
    "    # We evaluate against a random opponent\n",
    "    r = 0\n",
    "    for _ in range(n):\n",
    "        env.reset()\n",
    "        obs, reward, done, truncation, info = env.last()\n",
    "        while done == False:\n",
    "            p = env.agent_selection\n",
    "            \n",
    "            # Random\n",
    "            if p != player:\n",
    "                a = int(np.random.choice(np.nonzero(obs['action_mask'])[0], 1))\n",
    "                env.step(a)\n",
    "                obs, reward, done, truncation, info = env.last()\n",
    "                continue\n",
    "            \n",
    "            # Correct Player\n",
    "            state = torch.tensor(get_state(obs, player), device = device, dtype=torch.float32)\n",
    "            output = policy(torch.Tensor(state))\n",
    "            a = int(torch.argmax(output))\n",
    "            if obs['action_mask'][a] == 1:\n",
    "                env.step(a)\n",
    "                r += env.rewards[p]\n",
    "            else:\n",
    "                done = True\n",
    "                continue\n",
    "                \n",
    "            obs, reward, done, truncation, info = env.last()\n",
    "    r /= n\n",
    "    return r\n",
    "\n",
    "def evaluate_random(env, n, player = 'player_1'):\n",
    "    r = 0\n",
    "    for _ in range(n):\n",
    "        env.reset()\n",
    "        obs, reward, done, truncation, info = env.last()\n",
    "        while done == False:\n",
    "            p = env.agent_selection\n",
    "            a = int(np.random.choice(np.nonzero(obs['action_mask'])[0], 1))\n",
    "            env.step(a)\n",
    "            obs, reward, done, truncation, info = env.last()\n",
    "            print(p, reward)\n",
    "            print(env.rewards)\n",
    "            \n",
    "    return r/n\n",
    "    \n",
    "def select_action(state, policy, eps_tresh, mask, greedy=False):\n",
    "    assert state.shape[0] == 1\n",
    "    assert state.shape[1] == 9\n",
    "    sample = random.random()\n",
    "    if greedy == True or sample>eps_tresh:\n",
    "        state = torch.tensor(state, device = device, dtype = torch.float32)\n",
    "        legal_actions = np.nonzero(mask)[0]\n",
    "        action = int(torch.argmax(policy(state)[0,legal_actions]))\n",
    "        return action\n",
    "    else:\n",
    "        return int(np.random.choice(np.nonzero(mask)[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f23e38a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2984db970af2472c879b15d3a9cc159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35df1125200e48ebaf80bc8ee4843878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d85d8a8e6d40e982d77216cdc1aee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f1bdfdf0ea4534807f81acda2097b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3525534/1213792839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtraining_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3525534/1514517038.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Batch size larger than capacity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'player_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    468\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_steps = 0\n",
    "evals = {'player_1':[], 'player_2':[]}\n",
    "players = env.agents\n",
    "previous_state = {}\n",
    "previous_action = {}\n",
    "previous_reward = {}\n",
    "progress_bar = tqdm(range(episodes), desc = \"Episodes\", position = 0, leave = True)\n",
    "epsilon_bar = tqdm(bar_format = '{desc}', position = 1)\n",
    "p1_bar = tqdm(bar_format = '{desc}', position = 2)\n",
    "p2_bar = tqdm(bar_format = '{desc}', position = 3)\n",
    "bars = {'player_1': p1_bar, 'player_2': p2_bar}\n",
    "\n",
    "for steps_done in progress_bar:\n",
    "    \n",
    "    for p in players:\n",
    "        previous_action[p] = -1\n",
    "        previous_reward[p] = 0\n",
    "    \n",
    "    states = []\n",
    "        \n",
    "    env.reset()\n",
    "    obs, reward, done, truncation, info = env.last()\n",
    "    state = get_state(obs, 'player_1')\n",
    "    states.append(state)\n",
    "    a = select_action(state, policy_nets['player_1'], 1, obs['action_mask'])\n",
    "    previous_action['player_1'] = a\n",
    "    previous_reward['player_1'] = 0\n",
    "    env.step(a)\n",
    "    obs, reward, done, truncation, info = env.last()\n",
    "\n",
    "    i = 1\n",
    "    ended = True\n",
    "    while done == False:\n",
    "        \n",
    "        p = env.agent_selection\n",
    "        state = get_state(obs, p)\n",
    "        states.append(state)\n",
    "        \n",
    "        if i >= 2:\n",
    "            memories[p].push(states[i-2], previous_action[p], states[i], previous_reward[p])\n",
    "        \n",
    "        eps = max(eps_start * np.exp(rate*steps_done), eps_end)\n",
    "        a = select_action(state, policy_nets[p], eps, obs['action_mask'])\n",
    "        \n",
    "        # Legal action\n",
    "        if obs['action_mask'][a] == 1:\n",
    "            env.step(a)\n",
    "            r = env.rewards[p]\n",
    "            \n",
    "        # Illegal action\n",
    "        else:\n",
    "            done = True\n",
    "            memories[p].push(states[i], a, states[i], -1)\n",
    "            ended = False\n",
    "            continue\n",
    "            \n",
    "        obs, reward, done, truncation, info = env.last()\n",
    "        \n",
    "        previous_action[p] = a\n",
    "        previous_reward[p] = r\n",
    "        \n",
    "        time_steps += 1\n",
    "        i += 1       \n",
    "        \n",
    "        if time_steps%training_freq==0:\n",
    "            if len(memories[p]) >= batch_size:\n",
    "                batch = memories[p].sample(batch_size)\n",
    "            else:\n",
    "                continue\n",
    "            target_input = torch.empty(batch_size, 9, device=device)\n",
    "            policy_input = torch.empty(batch_size, 9, device = device)\n",
    "            rewards = torch.empty(batch_size,device = device)\n",
    "            actions = np.zeros(batch_size)\n",
    "            for ind, t in enumerate(batch):\n",
    "                policy_input[ind,:] = torch.tensor(t.state)\n",
    "                target_input[ind,:] = torch.tensor(t.next_state)\n",
    "                rewards[ind] = t.reward\n",
    "                actions[ind] = t.action\n",
    "            \n",
    "            q_values = policy_nets[p](policy_input)[np.arange(batch_size), actions]\n",
    "            next_state_q_values = torch.max(target_nets[p](target_input), 1)[0]*gamma + rewards[:]\n",
    "            loss = crits[p](q_values, next_state_q_values)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_nets[p].parameters(), max_norm = 1, norm_type = 2)\n",
    "            optimizers[p].step()\n",
    "            optimizers[p].zero_grad()\n",
    "            \n",
    "        if time_steps % target_update_freq == 0:\n",
    "            for p, net in target_nets.items():\n",
    "                net.load_state_dict(policy_nets[p].state_dict())\n",
    "        \n",
    "    # Update terminal states:\n",
    "    if ended:\n",
    "        _ = env.agent_selection\n",
    "        terminal = get_state(obs, _)\n",
    "        # update player_1\n",
    "        if p == \"player_1\": # final move made by player_1\n",
    "            memories[\"player_1\"].push(states[-1], previous_action[\"player_1\"], terminal, env.rewards[\"player_1\"])\n",
    "            memories[\"player_2\"].push(states[-2], previous_action[\"player_2\"], terminal, env.rewards[\"player_2\"])\n",
    "        else: # final move made by player_2\n",
    "            memories[\"player_1\"].push(states[-2], previous_action[\"player_1\"], terminal, env.rewards[\"player_1\"])\n",
    "            memories[\"player_2\"].push(states[-1], previous_action[\"player_2\"], terminal, env.rewards[\"player_2\"])\n",
    "        \n",
    "    if steps_done%eval_freq == 0:\n",
    "        for p in evals.keys():\n",
    "            e = evaluate(env, eval_episodes, policy_nets[p], p)\n",
    "            bars[p].set_description_str(f'{p} win rate = {e*100}%')\n",
    "            evals[p].append(e)\n",
    "        epsilon_bar.set_description_str(f'epsilon = {eps}')\n",
    "\n",
    "    if steps_done%save_freq == 0:\n",
    "        for p in evals.keys():\n",
    "            torch.save(policy_nets[p].state_dict(), f\"nn_params/TTT_{p}_policy_network_state_dict.pt\")\n",
    "            np.save(f\"nn_params/TTT_{p}_evaluations.py\", evals[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9bc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[269.9980, 257.8204, 257.2065, 231.2742, 276.9026, 240.5143, 231.4798,\n",
       "         214.4390, 245.5986]], grad_fn=<LeakyReluBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_nets['player_2'](np.array([[0,0,0,0,0,1,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9c6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1734e-05, -1.0977e-05, -5.9294e-05, -1.8750e-05,  1.7679e-05,\n",
      "          6.3044e-05, -6.3339e-05,  2.2786e-05, -1.7977e-05],\n",
      "        [ 6.3012e-05, -1.0142e-05,  1.0522e-05, -1.0185e-05,  1.0203e-05,\n",
      "          9.5310e-06,  5.9029e-07, -8.2633e-05,  7.2588e-05],\n",
      "        [ 8.1377e-04,  8.1429e-04, -7.8897e-04,  8.2483e-04, -8.2482e-04,\n",
      "         -8.1852e-04,  4.2058e-06,  7.8406e-04,  1.9604e-05],\n",
      "        [-1.9794e-03, -1.9452e-03,  1.9585e-03, -1.9318e-03,  1.9318e-03,\n",
      "          1.9586e-03, -1.3434e-05, -1.9590e-03,  4.3454e-07],\n",
      "        [ 3.6718e-05, -3.9206e-06,  7.8065e-05,  3.4602e-05, -3.3649e-05,\n",
      "          1.0265e-05,  3.7683e-06, -1.1349e-04,  6.2840e-05],\n",
      "        [ 1.9009e-04,  1.9046e-04, -1.9007e-04,  1.9034e-04, -1.9033e-04,\n",
      "         -1.9095e-04,  7.2903e-07,  1.9034e-04,  4.7874e-08],\n",
      "        [-3.0350e-03, -3.0365e-03,  3.0363e-03, -3.0365e-03,  3.0365e-03,\n",
      "          3.0364e-03, -2.9951e-07, -3.0372e-03,  9.8304e-07],\n",
      "        [ 1.2937e-05,  5.6012e-05, -1.4292e-04, -9.2410e-06,  9.2286e-06,\n",
      "         -1.2108e-04,  4.2196e-05,  2.2883e-04, -8.6036e-05],\n",
      "        [ 1.3201e-03,  1.3437e-03, -1.3471e-03,  1.3444e-03, -1.3435e-03,\n",
      "         -1.3455e-03,  2.9997e-06,  1.3469e-03, -1.7527e-06],\n",
      "        [ 9.4427e-04,  1.0754e-03, -1.0449e-03,  1.1091e-03, -1.1083e-03,\n",
      "         -1.0438e-03, -3.1717e-05,  1.1306e-03, -8.7301e-05],\n",
      "        [-1.1537e-04, -2.9119e-05,  9.2570e-05,  3.4314e-05, -3.4310e-05,\n",
      "          9.2896e-05, -6.3343e-05, -8.2942e-05, -9.9932e-06],\n",
      "        [-2.7416e-06, -1.2247e-05,  8.4567e-05,  1.6929e-05, -1.6861e-05,\n",
      "          4.7457e-06,  1.3922e-05, -8.6314e-05,  3.8082e-05],\n",
      "        [-6.0878e-03, -6.0129e-03,  6.1218e-03, -5.9754e-03,  5.9754e-03,\n",
      "          5.9786e-03,  3.1446e-05, -6.0014e-03, -4.8482e-05],\n",
      "        [-8.6458e-05, -1.0430e-05,  9.1150e-06, -9.0487e-06,  9.7358e-06,\n",
      "          9.7903e-06,  7.3042e-07,  5.7425e-05, -6.7956e-05],\n",
      "        [-4.9417e-05,  7.2583e-06,  1.1025e-05,  3.4316e-05, -3.2153e-05,\n",
      "          1.3559e-05, -2.0667e-05, -1.3160e-05, -2.4354e-06],\n",
      "        [-4.6642e-03, -4.6606e-03,  4.6196e-03, -4.6885e-03,  4.6887e-03,\n",
      "          4.6441e-03,  1.5632e-05, -4.6029e-03, -2.9401e-05],\n",
      "        [-2.1830e-05, -2.1556e-05,  4.4339e-05, -2.1729e-05,  2.1730e-05,\n",
      "         -1.6421e-06,  2.3116e-05, -2.0883e-05, -4.3640e-07],\n",
      "        [ 6.0860e-04,  6.0816e-04, -6.0813e-04,  6.0831e-04, -6.0831e-04,\n",
      "         -6.0773e-04, -2.6229e-07,  6.0714e-04,  7.1389e-07],\n",
      "        [-8.0918e-06, -8.8617e-06, -1.0519e-05, -1.0228e-05,  1.0226e-05,\n",
      "          7.6392e-06, -1.6648e-05,  1.0772e-05, -3.8506e-07],\n",
      "        [ 2.7776e-03,  2.8175e-03, -2.8108e-03,  2.8236e-03, -2.8236e-03,\n",
      "         -2.8117e-03, -5.4400e-06,  2.8114e-03, -4.1723e-07],\n",
      "        [-2.9851e-05, -3.1423e-05,  3.2191e-05, -3.1157e-05,  3.1144e-05,\n",
      "          3.1500e-05,  1.1636e-07, -3.3686e-05,  1.7509e-06],\n",
      "        [ 1.4217e-03,  1.4097e-03, -1.4227e-03,  1.3971e-03, -1.3971e-03,\n",
      "         -1.4222e-03,  1.2033e-05,  1.4231e-03, -3.7599e-07],\n",
      "        [ 1.7689e-05,  1.6119e-05, -3.5722e-05, -8.1632e-06,  8.1639e-06,\n",
      "         -4.0033e-05,  2.8978e-05,  4.9283e-05, -1.3934e-05],\n",
      "        [-4.6393e-05,  1.6570e-05, -3.3995e-05,  1.7027e-05, -1.7005e-05,\n",
      "         -1.6227e-05, -1.5307e-05,  5.4119e-05, -2.0120e-05],\n",
      "        [-2.7771e-03, -2.7554e-03,  2.8175e-03, -2.7328e-03,  2.7328e-03,\n",
      "          2.7384e-03,  1.6503e-05, -2.7788e-03,  8.0972e-07],\n",
      "        [-1.5579e-03, -1.6034e-03,  1.6002e-03, -1.6069e-03,  1.6069e-03,\n",
      "          1.5997e-03,  3.5601e-06, -1.6012e-03,  1.3686e-06],\n",
      "        [-2.2413e-03, -2.1529e-03,  2.2011e-03, -2.1292e-03,  2.1272e-03,\n",
      "          2.1786e-03, -1.4181e-05, -2.1245e-03, -6.8529e-05],\n",
      "        [-8.5183e-05, -2.9855e-05,  8.7892e-05,  2.2490e-05, -2.3816e-05,\n",
      "          8.6027e-05, -5.5931e-05, -8.7524e-05,  2.4318e-06],\n",
      "        [ 5.4274e-06,  6.5887e-06, -6.5828e-06,  6.5703e-06, -6.6987e-06,\n",
      "         -5.7584e-06, -8.5004e-07,  6.4366e-06, -6.0660e-08],\n",
      "        [ 4.5447e-05,  4.0795e-05, -4.0755e-05,  4.0127e-05, -4.0312e-05,\n",
      "         -4.0919e-05,  8.3546e-08,  3.6394e-05,  4.7426e-06],\n",
      "        [-1.4938e-05, -2.1387e-05,  1.5340e-05, -2.7443e-05,  2.7439e-05,\n",
      "          1.5336e-05,  6.0212e-06, -1.5555e-05,  2.2891e-07],\n",
      "        [ 1.9308e-05, -1.3467e-05, -3.4849e-05, -3.0887e-05,  3.0134e-05,\n",
      "          3.2401e-05, -1.8095e-05,  1.6406e-06,  6.1936e-07],\n",
      "        [-1.3575e-04,  1.5391e-05, -6.9013e-05,  2.0470e-05, -1.7804e-05,\n",
      "          8.3759e-06, -4.5091e-05,  1.4444e-04, -1.0745e-04],\n",
      "        [ 1.0307e-04, -8.2817e-06,  8.6406e-06, -7.8706e-06,  7.8732e-06,\n",
      "          8.9318e-06, -4.4213e-07, -1.2060e-04,  1.1171e-04],\n",
      "        [ 1.2112e-05,  1.3304e-05, -1.3358e-05,  1.3236e-05, -1.3220e-05,\n",
      "         -1.3473e-05,  2.0200e-07,  1.4576e-05, -1.1949e-06],\n",
      "        [-1.1907e-03, -1.1803e-03,  1.1878e-03, -1.1830e-03,  1.1807e-03,\n",
      "          1.2068e-03, -4.8881e-06, -1.2266e-03,  2.1203e-05],\n",
      "        [ 1.9073e-05,  1.9116e-05, -5.4763e-05,  2.0513e-05, -1.9863e-05,\n",
      "         -1.9262e-05, -3.3880e-05,  5.4358e-05, -1.2989e-06],\n",
      "        [ 1.8833e-03,  1.8831e-03, -1.8836e-03,  1.8831e-03, -1.8831e-03,\n",
      "         -1.8829e-03, -2.9017e-07,  1.8831e-03,  1.0777e-07],\n",
      "        [ 5.4121e-06,  2.2920e-07,  2.3453e-05, -4.5664e-06,  4.5666e-06,\n",
      "         -4.8812e-06,  3.3970e-05, -2.4054e-05,  4.5633e-07],\n",
      "        [-9.8567e-06, -4.6596e-05,  4.9211e-05, -4.8637e-05,  4.7416e-05,\n",
      "          4.8633e-05, -1.8628e-06, -8.6958e-05,  3.9776e-05],\n",
      "        [-1.1903e-05,  2.0867e-05, -3.6135e-05, -8.3020e-06,  8.2982e-06,\n",
      "         -7.6662e-05,  4.3145e-05,  1.2739e-04, -6.4606e-05],\n",
      "        [ 1.2426e-03,  1.2567e-03, -1.2472e-03,  1.2721e-03, -1.2691e-03,\n",
      "         -1.2566e-03,  4.6204e-07,  1.2500e-03, -2.7818e-06],\n",
      "        [-1.6743e-04, -6.9127e-06,  2.5386e-05,  1.5423e-05, -1.4563e-05,\n",
      "          2.7157e-05, -1.9391e-05,  7.2187e-05, -9.9783e-05],\n",
      "        [-1.8113e-05, -1.6571e-05,  4.6559e-05, -1.6558e-05,  1.6546e-05,\n",
      "          1.6658e-05,  2.9871e-05, -6.4625e-05,  1.8051e-05],\n",
      "        [ 5.3604e-04,  5.7722e-04, -5.4815e-04,  5.9205e-04, -5.9059e-04,\n",
      "         -5.7174e-04,  1.2290e-05,  6.0168e-04, -5.1502e-05],\n",
      "        [ 1.0563e-03,  9.5240e-04, -9.4064e-04,  9.3192e-04, -9.3191e-04,\n",
      "         -9.8634e-04,  5.2845e-05,  8.9650e-04,  5.7509e-05],\n",
      "        [ 6.8230e-05, -7.8258e-06, -5.3027e-06, -2.9446e-05,  2.7354e-05,\n",
      "         -7.7571e-06,  1.5428e-05, -4.1581e-05,  5.1308e-05],\n",
      "        [-9.5880e-06, -2.3449e-05,  1.0371e-04,  7.9911e-06, -9.0820e-06,\n",
      "          2.9448e-05,  9.2725e-06, -9.9130e-05,  2.6313e-05],\n",
      "        [ 2.9547e-03,  2.9550e-03, -2.9536e-03,  2.9531e-03, -2.9539e-03,\n",
      "         -2.9545e-03, -4.8846e-07,  2.9544e-03,  8.6203e-07],\n",
      "        [-2.7670e-05, -3.9597e-05,  8.3026e-05, -1.5255e-05,  1.4089e-05,\n",
      "          8.8716e-05, -1.4230e-05, -1.4291e-04,  4.0933e-05],\n",
      "        [ 1.5118e-03,  1.5685e-03, -1.5573e-03,  1.5683e-03, -1.5683e-03,\n",
      "         -1.5688e-03,  1.1832e-05,  1.5951e-03, -3.7878e-05],\n",
      "        [-2.3323e-03, -2.3652e-03,  2.3654e-03, -2.3654e-03,  2.3654e-03,\n",
      "          2.3649e-03, -3.8857e-07, -2.3655e-03,  3.6365e-07],\n",
      "        [ 4.7445e-03,  4.7441e-03, -4.6883e-03,  4.7439e-03, -4.7439e-03,\n",
      "         -4.8004e-03,  5.7352e-05,  4.7440e-03,  2.9119e-07],\n",
      "        [-3.0227e-03, -2.9489e-03,  2.9418e-03, -2.9389e-03,  2.9389e-03,\n",
      "          2.9765e-03, -2.6744e-05, -2.9233e-03, -3.6037e-05],\n",
      "        [-7.1509e-03, -7.1097e-03,  7.1284e-03, -7.0916e-03,  7.0916e-03,\n",
      "          7.1277e-03, -1.7689e-05, -7.1135e-03, -1.4743e-05],\n",
      "        [ 9.4624e-05, -1.6726e-05,  1.4950e-05, -1.0435e-05,  9.1961e-06,\n",
      "          3.0749e-05, -2.3644e-05, -1.2960e-04,  1.1311e-04],\n",
      "        [-3.0189e-03, -2.9997e-03,  2.9987e-03, -2.9987e-03,  2.9992e-03,\n",
      "          2.9992e-03,  5.4463e-07, -2.9981e-03, -1.7352e-06],\n",
      "        [ 2.6503e-05,  1.0226e-05, -5.4119e-05, -5.5950e-06,  5.5974e-06,\n",
      "         -1.2404e-05, -1.2231e-05,  4.0443e-05,  2.0495e-08],\n",
      "        [ 2.0453e-03,  2.0736e-03, -2.0464e-03,  2.1008e-03, -2.1008e-03,\n",
      "         -2.0462e-03, -2.7290e-05,  2.0466e-03, -3.8652e-07],\n",
      "        [-3.1473e-03, -3.1262e-03,  3.1486e-03, -3.1040e-03,  3.1040e-03,\n",
      "          3.1486e-03, -2.2129e-05, -3.1499e-03,  1.1820e-06],\n",
      "        [-1.0823e-04,  3.8778e-06, -1.1103e-05,  4.4079e-06, -4.2106e-06,\n",
      "         -3.8903e-06, -6.1814e-06,  1.0033e-04, -8.9677e-05],\n",
      "        [ 5.1269e-07, -2.0056e-05,  1.5262e-05, -2.0526e-05,  2.0393e-05,\n",
      "          2.6352e-05, -4.7911e-06, -4.2075e-05,  2.0713e-05],\n",
      "        [ 1.5629e-04,  2.7412e-07,  4.6268e-05, -1.0892e-06,  4.0848e-07,\n",
      "         -3.6109e-05,  4.4497e-05, -1.6572e-04,  1.5733e-04],\n",
      "        [ 2.0905e-05,  2.1911e-05, -2.2932e-05,  2.1590e-05, -2.1583e-05,\n",
      "         -2.1915e-05, -3.1236e-07,  2.3746e-05, -1.1650e-06]])\n"
     ]
    }
   ],
   "source": [
    "print(policy_nets['player_2'].model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ede7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
