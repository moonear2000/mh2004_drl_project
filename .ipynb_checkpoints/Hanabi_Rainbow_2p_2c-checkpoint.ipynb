{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017a78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.classic import hanabi_v4\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    RainbowPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy,\n",
    ")\n",
    "from tianshou.utils.net.discrete import NoisyLinear\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856220c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of rainbow used in deepmind paper\n",
    "p = {\n",
    "    'hidden_layers': [256,256],\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-4,\n",
    "    'target_update_freq': 500,\n",
    "    'estimation_steps': 1,\n",
    "    'num_train':32,\n",
    "    'num_test':16,\n",
    "    'buffer_size':50000,\n",
    "    'vmax':25,\n",
    "    'vmin':-25,\n",
    "    'noisy_std':0.1,\n",
    "    'atom_size':51,\n",
    "    'minimum_replay_history':500,\n",
    "    'batch_size':32,\n",
    "    'steps_per_collect': 4,\n",
    "    'updates_per_train': 1,\n",
    "    'test_steps': 10000,\n",
    "    'epochs':int(10e6),\n",
    "    'eps_decay_period': 1000,\n",
    "    'test_frequency': 1000,\n",
    "    'test_eps': 0,\n",
    "    'save_frequency': 10000,\n",
    "    'eps_final':0.01,\n",
    "    'adam_eps': 3.125e-5,\n",
    "    'path': 'saved_data/rainbow_hanabi_small/',\n",
    "    'lr_scheduler_factor': 0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1b13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(render_mode=None):\n",
    "    return PettingZooEnv(hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=2, max_information_tokens=3,\n",
    "max_life_tokens=1, observation_type=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14b47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents(p):\n",
    "    \n",
    "    def noisy_linear(x, y):\n",
    "        return NoisyLinear(x, y, p['noisy_std'])\n",
    "    \n",
    "    # Return Policy, Agents, Envs\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(\n",
    "    env.observation_space, gym.spaces.Dict\n",
    "    ) else env.observation_space\n",
    "\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net = Net(\n",
    "            state_shape,\n",
    "            action_shape,\n",
    "            hidden_sizes=p['hidden_layers'],\n",
    "            device = device,\n",
    "            softmax = True,\n",
    "            num_atoms = p['atom_size'],\n",
    "            dueling_param = ({\n",
    "                'linear_layer': noisy_linear\n",
    "            }, {\n",
    "                'linear_layer': noisy_linear})\n",
    "    )\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr= p['lr'], eps=p['adam_eps'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode = 'max', factor = p['lr_scheduler_factor'],\n",
    "                                                              patience = 10)\n",
    "\n",
    "    agent = RainbowPolicy(\n",
    "            net,\n",
    "            optim,\n",
    "            p['gamma'],\n",
    "            num_atoms = p['atom_size'],\n",
    "            v_min = p['vmin'],\n",
    "            v_max = p['vmax'],\n",
    "            estimation_step = p['estimation_steps'],\n",
    "            target_update_freq=p['target_update_freq']\n",
    "        ).to(device)\n",
    "\n",
    "    agents = [agent, agent]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    agents = env.agents\n",
    "\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(p['num_train'])])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(p['num_test'])])\n",
    "    \n",
    "    return policy, agents, train_envs, test_envs, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b396937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collectors(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    test_envs,\n",
    "    p\n",
    "):\n",
    "    \n",
    "    # Get collectors\n",
    "    train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    PrioritizedVectorReplayBuffer(p['buffer_size'], len(train_envs), alpha = 0.6, beta = 0.4, weight_norm=True),\n",
    "    exploration_noise=True)\n",
    "    \n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    \n",
    "    return train_collector, test_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(\n",
    "    train_collector,\n",
    "    agents,\n",
    "    policy,\n",
    "    p\n",
    "):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(1)\n",
    "    train_collector.collect(n_step = p['minimum_replay_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13665eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy(policy, agents, p):\n",
    "    for a in agents:\n",
    "        torch.save(policy.policies[a].state_dict(), f'{p[\"path\"]}{a}_params.pth')\n",
    "\n",
    "def save_history(history, p):\n",
    "    np.save(f'{p[\"path\"]}training_rewards.npy', np.array(history))\n",
    "    \n",
    "def change_lr(optimizer, new_lr):\n",
    "    # Run this to change the learning rate to 1e-5:\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b85c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps(iteration, p):\n",
    "    if iteration > p['eps_decay_period']:\n",
    "        return p['eps_final']\n",
    "    else:\n",
    "        gradient = (1 - p['eps_final'])/p['eps_decay_period']\n",
    "        return 1 - gradient*iteration\n",
    "        \n",
    "def set_eps(policy, agents, new_eps):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(new_eps)\n",
    "        \n",
    "def train(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    agents,\n",
    "    p,\n",
    "    lr_scheduler,\n",
    "    training_history = []\n",
    "):\n",
    "\n",
    "    for i in tqdm(range(p['epochs'])):\n",
    "        \n",
    "        eps = get_eps(i, p)\n",
    "        set_eps(policy, agents, eps)\n",
    "        \n",
    "        # Collection step\n",
    "        result = train_collector.collect(n_step = p['steps_per_collect'])\n",
    "        \n",
    "        # Test Step\n",
    "        if i%p['test_frequency'] == 0:\n",
    "            set_eps(policy, agents, p['test_eps'])\n",
    "            result = test_collector.collect(n_step = p['test_steps'])\n",
    "            mean_reward = result['rews'].mean()\n",
    "            tqdm.write(str(mean_reward))\n",
    "            training_history.append(mean_reward)\n",
    "            set_eps(policy, agents, eps)\n",
    "            lr_scheduler.step(mean_reward)\n",
    "    \n",
    "        if i%p['save_frequency'] == 0:\n",
    "            save_policy(policy, agents,p)\n",
    "            save_history(training_history,p)\n",
    "            plot_and_save(training_history, p['test_frequency'],p, show = False)\n",
    "    \n",
    "        # Update step (one epoch)\n",
    "        for _ in range(p['updates_per_train']): \n",
    "            losses = policy.update(p['batch_size'], train_collector.buffer)\n",
    "    \n",
    "    plot_and_save(training_history, test_frequency)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a19b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(training_history, test_frequency, p, save = True, show = True):\n",
    "    x = np.arange(len(training_history))\n",
    "    x *= test_frequency\n",
    "    plt.plot(x, training_history)\n",
    "    plt.title('Combined Average Score (Rainbow, 2 Color game)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Score (max 10)')\n",
    "    if save: plt.savefig(f'{p[\"path\"]}training_curve.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "def load(policy, agents, p):\n",
    "    for a in agents:\n",
    "        policy.policies[a].load_state_dict(torch.load(f'{p[\"path\"]}{a}_params.pth'))\n",
    "    his = list(np.load(f'{p[\"path\"]}training_rewards.npy'))\n",
    "    return his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf68087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/collector.py:255: UserWarning: n_step=500 is not a multiple of #env (32), which may cause extra transitions collected into the buffer.\n",
      "  warnings.warn(\n",
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n",
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy, agents, train_envs, test_envs, lr_scheduler = get_agents(p)\n",
    "train_collector, test_collector = get_collectors(policy, train_envs, test_envs, p)\n",
    "initialize_buffer(train_collector, agents, policy, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f614d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_history = load(policy, agents,p)\n",
    "training_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c545f926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632a1f02d1c84a26a8a82260472a1858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/collector.py:255: UserWarning: n_step=4 is not a multiple of #env (32), which may cause extra transitions collected into the buffer.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21396396396396397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/collector.py:255: UserWarning: n_step=4 is not a multiple of #env (32), which may cause extra transitions collected into the buffer.\n",
      "  warnings.warn(\n",
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5458333333333333\n",
      "0.7407407407407407\n",
      "1.0170068027210883\n",
      "0.8825396825396825\n",
      "1.2673267326732673\n",
      "1.2452229299363058\n",
      "1.2453416149068324\n",
      "0.9369085173501577\n",
      "1.2535612535612535\n",
      "1.160458452722063\n",
      "1.1913439635535308\n",
      "1.1717171717171717\n",
      "1.2\n",
      "1.2222222222222223\n",
      "1.2729591836734695\n",
      "1.4436860068259385\n",
      "1.0769230769230769\n",
      "1.255813953488372\n",
      "1.4625407166123778\n",
      "1.3647798742138364\n",
      "1.5906040268456376\n",
      "1.5394321766561514\n",
      "1.763157894736842\n",
      "1.7922077922077921\n",
      "1.8417721518987342\n",
      "1.7258566978193146\n",
      "1.7350993377483444\n",
      "1.950657894736842\n",
      "1.7130681818181819\n",
      "1.8123324396782843\n",
      "1.957516339869281\n",
      "1.891025641025641\n",
      "2.1024844720496896\n",
      "1.9516616314199395\n",
      "2.2315112540192925\n",
      "2.5728155339805827\n",
      "2.084084084084084\n",
      "2.11\n",
      "2.3854748603351954\n",
      "2.2036474164133737\n",
      "2.6018808777429467\n",
      "2.5047619047619047\n",
      "2.2507552870090635\n",
      "2.3084415584415585\n",
      "2.3669467787114846\n",
      "2.2605042016806722\n",
      "2.1171171171171173\n",
      "2.211038961038961\n",
      "2.462025316455696\n",
      "2.6807909604519775\n",
      "2.2493074792243766\n",
      "2.4684931506849317\n",
      "2.710691823899371\n",
      "2.453731343283582\n",
      "2.8328267477203646\n",
      "2.738853503184713\n",
      "3.147909967845659\n",
      "3.3492063492063493\n",
      "2.842767295597484\n",
      "3.09009009009009\n",
      "3.335384615384615\n",
      "3.3184713375796178\n",
      "3.3890577507598785\n",
      "3.236923076923077\n",
      "3.2018348623853212\n",
      "3.5\n",
      "3.5828220858895707\n",
      "3.5917721518987342\n",
      "3.6826923076923075\n",
      "3.458204334365325\n",
      "3.6206896551724137\n",
      "3.7371794871794872\n",
      "3.362229102167183\n",
      "3.67192429022082\n",
      "3.641269841269841\n",
      "3.7318611987381702\n",
      "3.719242902208202\n",
      "3.697749196141479\n",
      "3.7859424920127798\n",
      "3.7770700636942673\n",
      "3.725552050473186\n",
      "3.3512658227848102\n",
      "3.7714285714285714\n",
      "3.6910828025477707\n",
      "3.7009646302250805\n",
      "3.652037617554859\n",
      "3.806349206349206\n",
      "3.8802588996763756\n",
      "3.8380952380952382\n",
      "3.804416403785489\n",
      "3.8108974358974357\n",
      "3.9341692789968654\n",
      "4.156739811912225\n",
      "3.9522292993630574\n",
      "4.072555205047319\n",
      "3.984567901234568\n",
      "4.147798742138365\n",
      "4.148148148148148\n",
      "4.151419558359621\n",
      "4.15987460815047\n",
      "4.3354037267080745\n",
      "4.333333333333333\n",
      "4.415094339622642\n",
      "4.292168674698795\n",
      "4.26984126984127\n",
      "4.186186186186186\n",
      "4.338509316770186\n",
      "4.4829721362229105\n",
      "4.484472049689441\n",
      "4.44\n",
      "4.264615384615385\n",
      "4.58204334365325\n",
      "4.5046728971962615\n",
      "4.18018018018018\n",
      "4.466463414634147\n",
      "4.4733542319749215\n",
      "4.410876132930514\n",
      "4.576687116564417\n",
      "4.5225225225225225\n",
      "4.571428571428571\n",
      "4.524242424242424\n",
      "4.489230769230769\n",
      "4.327272727272727\n",
      "4.472049689440993\n",
      "4.693939393939394\n",
      "4.641337386018237\n",
      "4.507552870090635\n",
      "4.756838905775076\n",
      "4.6419753086419755\n",
      "4.618461538461538\n",
      "4.564417177914111\n",
      "4.667701863354037\n",
      "4.641791044776119\n",
      "4.6894409937888195\n",
      "4.46583850931677\n",
      "4.562111801242236\n",
      "4.535825545171339\n",
      "4.646341463414634\n",
      "4.780564263322884\n",
      "4.648148148148148\n",
      "4.570121951219512\n",
      "4.613003095975232\n",
      "4.576687116564417\n",
      "4.625766871165644\n",
      "4.538699690402477\n",
      "4.670807453416149\n",
      "4.672839506172839\n",
      "4.8246153846153845\n",
      "4.703703703703703\n",
      "4.697819314641745\n",
      "4.70625\n",
      "4.798761609907121\n",
      "4.722222222222222\n",
      "4.8125\n",
      "4.431677018633541\n",
      "4.720496894409938\n",
      "4.765243902439025\n",
      "4.704968944099379\n",
      "4.674846625766871\n",
      "4.72360248447205\n",
      "4.705882352941177\n",
      "4.754601226993865\n",
      "4.705882352941177\n",
      "4.658385093167702\n",
      "4.634146341463414\n",
      "4.777777777777778\n",
      "4.7846153846153845\n",
      "4.777089783281734\n",
      "4.77639751552795\n",
      "4.7253086419753085\n",
      "4.750778816199377\n",
      "4.730061349693251\n",
      "4.787692307692308\n",
      "4.746913580246914\n",
      "4.773291925465839\n",
      "4.707692307692308\n",
      "4.770186335403727\n",
      "4.787037037037037\n",
      "4.773291925465839\n",
      "4.801242236024844\n",
      "4.87037037037037\n",
      "4.795665634674923\n",
      "4.698757763975156\n",
      "4.748466257668712\n",
      "4.557632398753894\n",
      "4.88\n",
      "4.763975155279503\n",
      "4.786377708978328\n",
      "4.804953560371517\n",
      "4.825153374233129\n",
      "4.677914110429448\n",
      "4.747663551401869\n",
      "4.763076923076923\n",
      "4.874233128834356\n",
      "4.74223602484472\n",
      "4.827160493827161\n",
      "4.839506172839506\n",
      "4.829721362229102\n",
      "4.780185758513932\n",
      "4.788343558282208\n",
      "4.841121495327103\n",
      "4.748447204968944\n",
      "4.795031055900621\n",
      "4.804281345565749\n",
      "4.793103448275862\n",
      "4.798761609907121\n",
      "4.790123456790123\n",
      "4.756923076923077\n",
      "4.7632398753894085\n",
      "4.786377708978328\n",
      "4.7392638036809815\n",
      "4.804347826086956\n",
      "4.835913312693498\n",
      "4.815384615384615\n",
      "4.791925465838509\n",
      "4.773291925465839\n",
      "4.904615384615385\n",
      "4.838509316770186\n",
      "4.809815950920245\n",
      "4.851393188854489\n",
      "4.770186335403727\n",
      "4.74922600619195\n",
      "4.74922600619195\n",
      "4.751533742331288\n",
      "4.769470404984424\n",
      "4.825545171339564\n",
      "4.814814814814815\n",
      "4.777089783281734\n",
      "4.850152905198777\n",
      "4.750778816199377\n",
      "4.773993808049536\n",
      "4.835913312693498\n",
      "4.824074074074074\n",
      "4.788161993769471\n",
      "4.7592592592592595\n",
      "4.768518518518518\n",
      "4.771604938271605\n",
      "4.772307692307693\n",
      "4.754658385093168\n",
      "4.770186335403727\n",
      "4.756923076923077\n",
      "4.773993808049536\n",
      "4.78328173374613\n",
      "4.796296296296297\n",
      "4.797507788161994\n",
      "4.858461538461539\n",
      "4.806748466257669\n",
      "4.798761609907121\n",
      "4.782608695652174\n",
      "4.6894409937888195\n",
      "4.6941896024464835\n",
      "4.782608695652174\n",
      "4.711180124223603\n",
      "4.757668711656442\n",
      "4.762345679012346\n",
      "4.779503105590062\n",
      "4.831775700934579\n",
      "4.9079754601226995\n",
      "4.863354037267081\n",
      "4.811145510835913\n",
      "4.762195121951219\n",
      "4.813664596273292\n",
      "4.845201238390093\n",
      "4.780864197530864\n",
      "4.711180124223603\n",
      "4.782208588957055\n",
      "4.787037037037037\n",
      "4.719626168224299\n",
      "4.746130030959752\n",
      "4.7746913580246915\n",
      "4.834890965732087\n",
      "4.8390092879256965\n",
      "4.8\n",
      "4.829192546583851\n",
      "4.775384615384615\n",
      "4.863777089783282\n",
      "4.790123456790123\n",
      "4.842105263157895\n",
      "4.894736842105263\n",
      "4.780185758513932\n",
      "4.735384615384615\n",
      "4.7912772585669785\n",
      "4.743034055727554\n",
      "4.820987654320987\n",
      "4.842105263157895\n",
      "4.763975155279503\n",
      "4.867692307692308\n",
      "4.867692307692308\n",
      "4.820433436532507\n",
      "4.7725856697819315\n",
      "4.736196319018405\n",
      "4.804953560371517\n",
      "4.832298136645963\n",
      "4.788161993769471\n",
      "4.834862385321101\n",
      "4.764705882352941\n",
      "4.833333333333333\n",
      "4.69969040247678\n",
      "4.795665634674923\n",
      "4.780564263322884\n",
      "4.796296296296297\n",
      "4.691358024691358\n",
      "4.851393188854489\n",
      "4.75776397515528\n",
      "4.854037267080745\n",
      "4.7027863777089784\n",
      "4.795665634674923\n",
      "4.833333333333333\n",
      "4.820433436532507\n",
      "4.711180124223603\n",
      "4.836923076923077\n",
      "4.819875776397516\n",
      "4.8390092879256965\n",
      "4.762345679012346\n",
      "4.788161993769471\n",
      "4.845679012345679\n",
      "4.7632398753894085\n",
      "4.783950617283951\n",
      "4.814814814814815\n",
      "4.818461538461539\n",
      "4.670846394984326\n",
      "4.855828220858895\n",
      "4.739130434782608\n",
      "4.867283950617284\n",
      "4.808049535603715\n",
      "4.736024844720497\n",
      "4.730061349693251\n",
      "4.802507836990595\n",
      "4.847560975609756\n",
      "4.757668711656442\n",
      "4.797507788161994\n",
      "4.7625\n",
      "4.728395061728395\n",
      "4.6923076923076925\n",
      "4.803076923076923\n",
      "4.682098765432099\n",
      "4.7725856697819315\n",
      "4.829721362229102\n",
      "4.773993808049536\n",
      "4.818461538461539\n",
      "4.786377708978328\n",
      "4.849230769230769\n",
      "4.805555555555555\n",
      "4.751552795031056\n",
      "4.808049535603715\n",
      "4.746913580246914\n",
      "4.795665634674923\n",
      "4.74922600619195\n",
      "4.722222222222222\n",
      "4.839506172839506\n",
      "4.777777777777778\n",
      "4.874233128834356\n",
      "4.86687306501548\n",
      "4.813084112149533\n",
      "4.785276073619632\n",
      "4.751552795031056\n",
      "4.798761609907121\n",
      "4.703125\n",
      "4.7213622291021675\n",
      "4.845201238390093\n",
      "4.855384615384615\n",
      "4.857142857142857\n",
      "4.771604938271605\n",
      "4.783950617283951\n",
      "4.697819314641745\n",
      "4.812307692307693\n",
      "4.730061349693251\n",
      "4.8473520249221185\n",
      "4.795031055900621\n",
      "4.877675840978593\n",
      "4.79320987654321\n",
      "4.813664596273292\n",
      "4.777089783281734\n",
      "4.708978328173375\n",
      "4.773993808049536\n",
      "4.803076923076923\n",
      "4.751552795031056\n",
      "4.820987654320987\n",
      "4.812307692307693\n",
      "4.705882352941177\n",
      "4.811728395061729\n",
      "4.781931464174455\n",
      "4.70679012345679\n",
      "4.833846153846154\n",
      "4.773291925465839\n",
      "4.76\n",
      "4.795665634674923\n",
      "4.796296296296297\n",
      "4.817337461300309\n",
      "4.845201238390093\n",
      "4.787037037037037\n",
      "4.754601226993865\n",
      "4.728395061728395\n",
      "4.811145510835913\n",
      "4.804953560371517\n",
      "4.780185758513932\n",
      "4.741433021806854\n",
      "4.848765432098766\n",
      "4.7592592592592595\n",
      "4.778816199376947\n",
      "4.838006230529595\n",
      "4.831288343558282\n",
      "4.791925465838509\n",
      "4.755351681957187\n",
      "4.811145510835913\n",
      "4.773993808049536\n",
      "4.826625386996904\n",
      "4.787037037037037\n",
      "4.8765432098765435\n",
      "4.800613496932515\n",
      "4.816199376947041\n",
      "4.761609907120743\n",
      "4.748447204968944\n",
      "4.8602484472049685\n",
      "4.865030674846626\n",
      "4.750778816199377\n",
      "4.824074074074074\n",
      "4.728395061728395\n",
      "4.830246913580247\n",
      "4.802469135802469\n",
      "4.814814814814815\n",
      "4.809815950920245\n",
      "4.890965732087228\n",
      "4.874233128834356\n",
      "4.827692307692308\n",
      "4.8229813664596275\n",
      "4.7625\n",
      "4.781538461538462\n",
      "4.8730650154798765\n",
      "4.888888888888889\n",
      "4.698757763975156\n",
      "4.755417956656347\n",
      "4.835913312693498\n",
      "4.795107033639144\n",
      "4.780185758513932\n",
      "4.759375\n",
      "4.756923076923077\n",
      "4.78328173374613\n",
      "4.879256965944273\n",
      "4.848765432098766\n",
      "4.825\n",
      "4.747692307692308\n",
      "4.798761609907121\n",
      "4.831775700934579\n",
      "4.818461538461539\n",
      "4.786377708978328\n",
      "4.8\n",
      "4.7446153846153845\n",
      "4.821538461538461\n",
      "4.795665634674923\n",
      "4.778816199376947\n",
      "4.733746130030959\n",
      "4.7975460122699385\n",
      "4.75625\n",
      "4.78328173374613\n",
      "4.659442724458204\n",
      "4.81039755351682\n",
      "4.785714285714286\n",
      "4.697819314641745\n",
      "4.755417956656347\n",
      "4.833333333333333\n",
      "4.808641975308642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.875\n",
      "4.833846153846154\n",
      "4.809968847352025\n",
      "4.8246153846153845\n",
      "4.798136645962733\n",
      "4.853211009174312\n",
      "4.872670807453416\n",
      "4.806153846153846\n",
      "4.735202492211838\n",
      "4.790123456790123\n",
      "4.8580246913580245\n",
      "4.773291925465839\n",
      "4.758513931888545\n",
      "4.700934579439252\n",
      "4.806153846153846\n",
      "4.827160493827161\n",
      "4.830246913580247\n",
      "4.7894736842105265\n",
      "4.814814814814815\n",
      "4.876160990712075\n",
      "4.814241486068111\n",
      "4.711180124223603\n",
      "4.763803680981595\n",
      "4.766355140186916\n",
      "4.782208588957055\n",
      "4.793846153846154\n",
      "4.773291925465839\n",
      "4.805555555555555\n",
      "4.838006230529595\n",
      "4.796296296296297\n",
      "4.71076923076923\n",
      "4.782608695652174\n",
      "4.787037037037037\n",
      "4.777089783281734\n",
      "4.66358024691358\n",
      "4.809968847352025\n",
      "4.739938080495356\n",
      "4.733746130030959\n",
      "4.790123456790123\n",
      "4.756172839506172\n",
      "4.745341614906832\n",
      "4.768518518518518\n",
      "4.795031055900621\n",
      "4.833846153846154\n",
      "4.770186335403727\n",
      "4.814241486068111\n",
      "4.720496894409938\n",
      "4.698757763975156\n",
      "4.79375\n",
      "4.846625766871166\n",
      "4.843076923076923\n",
      "4.804347826086956\n",
      "4.7407407407407405\n",
      "4.8478260869565215\n",
      "4.808049535603715\n",
      "4.875776397515528\n",
      "4.919753086419753\n",
      "4.803680981595092\n",
      "4.738317757009346\n",
      "4.874233128834356\n",
      "4.756172839506172\n",
      "4.857142857142857\n",
      "4.7625\n",
      "4.7592592592592595\n",
      "4.745341614906832\n",
      "4.820987654320987\n",
      "4.7894736842105265\n",
      "4.786377708978328\n",
      "4.760869565217392\n",
      "4.820987654320987\n",
      "4.817337461300309\n",
      "4.758513931888545\n",
      "4.827692307692308\n",
      "4.8229813664596275\n",
      "4.743034055727554\n",
      "4.6645962732919255\n",
      "4.833333333333333\n",
      "4.796296296296297\n",
      "4.771604938271605\n",
      "4.798136645962733\n",
      "4.730650154798762\n",
      "4.779141104294479\n",
      "4.814241486068111\n",
      "4.861111111111111\n",
      "4.767080745341615\n",
      "4.712962962962963\n",
      "4.764890282131661\n",
      "4.772307692307693\n",
      "4.7024539877300615\n",
      "4.829721362229102\n",
      "4.733128834355829\n",
      "4.798136645962733\n",
      "4.7708978328173375\n",
      "4.824074074074074\n",
      "4.766153846153846\n",
      "4.76\n",
      "4.7592592592592595\n",
      "4.82874617737003\n",
      "4.738317757009346\n",
      "4.767801857585139\n",
      "4.819018404907975\n",
      "4.780564263322884\n",
      "4.890577507598784\n",
      "4.842592592592593\n",
      "4.7875\n",
      "4.843076923076923\n",
      "4.792569659442725\n",
      "4.845201238390093\n",
      "4.731481481481482\n",
      "4.873846153846154\n",
      "4.792569659442725\n",
      "4.781538461538462\n",
      "4.819018404907975\n",
      "4.8229813664596275\n",
      "4.820433436532507\n",
      "4.7439024390243905\n",
      "4.678125\n",
      "4.777089783281734\n",
      "4.786377708978328\n",
      "4.879256965944273\n",
      "4.782608695652174\n",
      "4.772307692307693\n",
      "4.819571865443425\n",
      "4.72360248447205\n",
      "4.773993808049536\n",
      "4.812307692307693\n",
      "4.790625\n",
      "4.815384615384615\n",
      "4.803738317757009\n",
      "4.829268292682927\n",
      "4.7213622291021675\n",
      "4.758513931888545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111812/3111622494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_111812/3434042788.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(policy, train_collector, test_collector, agents, p, lr_scheduler, training_history)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Update step (one epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'updates_per_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mplot_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/base.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/multiagent/mapolicy.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/modelfree/rainbow.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# so that NoisyLinear takes effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/modelfree/c51.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prio-buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    294\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mstep_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(policy, train_collector, test_collector, agents, p, lr_scheduler, training_history = training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73832f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c9945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a02f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
