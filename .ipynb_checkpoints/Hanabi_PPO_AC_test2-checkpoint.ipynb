{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017a78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.classic import hanabi_v4\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    RainbowPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy,\n",
    ")\n",
    "from tianshou.utils.net.discrete import NoisyLinear\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856220c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'hidden_layers': [256,256],\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-6,\n",
    "    'target_update_freq': 200,\n",
    "    'estimation_steps': 1,\n",
    "    'num_train':50,\n",
    "    'num_test':50,\n",
    "    'buffer_size':50000,\n",
    "    'batch_size':32,\n",
    "    'steps_per_collect': 10000,\n",
    "    'updates_per_train': 500,\n",
    "    'test_steps': 50000,\n",
    "    'epochs':1000,\n",
    "    'test_frequency':5,\n",
    "    'save_frequency':25,\n",
    "    'eps':0.01\n",
    "}\n",
    "path = 'saved_data/training_group_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1b13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(render_mode=None):\n",
    "    return PettingZooEnv(hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=4, max_information_tokens=5,\n",
    "max_life_tokens=2, observation_type=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c14b47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents(\n",
    "    lr = 1e-4, \n",
    "    hidden_layers = [256, 256], \n",
    "    gamma = 0.99,\n",
    "    target_update_freq = 200, \n",
    "    estimation_steps = 1, \n",
    "    num_train = 50, \n",
    "    num_test = 50,\n",
    "    atom_size = 51,\n",
    "    noisy_std = 0.1\n",
    "):\n",
    "    \n",
    "    def noisy_linear(x, y):\n",
    "        return NoisyLinear(x, y, noisy_std)\n",
    "    \n",
    "    # Return Policy, Agents, Envs\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(\n",
    "    env.observation_space, gym.spaces.Dict\n",
    "    ) else env.observation_space\n",
    "\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net1 = Net(\n",
    "            state_shape,\n",
    "            action_shape,\n",
    "            hidden_sizes=hidden_layers,\n",
    "            device = device,\n",
    "            softmax = True,\n",
    "            num_atoms = atom_size,\n",
    "            dueling_param = ({\n",
    "                'linear_layer': noisy_linear\n",
    "            }, {\n",
    "                'linear_layer': noisy_linear})\n",
    "    )\n",
    "\n",
    "    net2 = Net(\n",
    "            state_shape,\n",
    "            action_shape,\n",
    "            hidden_sizes=hidden_layers,\n",
    "            device = device,\n",
    "            softmax = True,\n",
    "            num_atoms = atom_size,\n",
    "            dueling_param = ({\n",
    "                'linear_layer': noisy_linear\n",
    "            }, {\n",
    "                'linear_layer': noisy_linear})\n",
    "    )\n",
    "\n",
    "    optim1 = torch.optim.Adam(net1.parameters(), lr= lr)\n",
    "    optim2 = torch.optim.Adam(net2.parameters(), lr = lr)\n",
    "\n",
    "    agent1 = RainbowPolicy(\n",
    "            net1,\n",
    "            optim1,\n",
    "            gamma,\n",
    "            num_atoms = atom_size,\n",
    "            v_min = -10,\n",
    "            v_max = 10,\n",
    "            estimation_step = estimation_steps,\n",
    "            target_update_freq=target_update_freq\n",
    "        ).to(device)\n",
    "\n",
    "    agent2 = RainbowPolicy(\n",
    "            net2,\n",
    "            optim2,\n",
    "            gamma,\n",
    "            num_atoms = atom_size,\n",
    "            v_min = -10,\n",
    "            v_max = 10,\n",
    "            estimation_step = estimation_steps,\n",
    "            target_update_freq=target_update_freq\n",
    "        ).to(device)\n",
    "\n",
    "    agents = [agent1, agent1]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    agents = env.agents\n",
    "\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(num_train)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(num_test)])\n",
    "    \n",
    "    return policy, agents, train_envs, test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b396937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collectors(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    test_envs,\n",
    "    buffer_size\n",
    "):\n",
    "    \n",
    "    # Get collectors\n",
    "    train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    PrioritizedVectorReplayBuffer(buffer_size, len(train_envs), alpha = 0.6, beta = 0.4, weight_norm=True),\n",
    "    exploration_noise=True)\n",
    "    \n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    \n",
    "    return train_collector, test_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79e1b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(\n",
    "    train_collector,\n",
    "    buffer_size,\n",
    "    agents,\n",
    "    policy,\n",
    "    eps = 0.1\n",
    "):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(1)\n",
    "    train_collector.collect(n_step = buffer_size)\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13665eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy(policy, agents):\n",
    "    for a in agents:\n",
    "        torch.save(policy.policies[a].state_dict(), f'saved_data/training_group_2/{a}_params.pth')\n",
    "\n",
    "def save_history(history):\n",
    "    np.save(f'saved_data/training_group_2/training_rewards.npy', np.array(history))\n",
    "    \n",
    "def change_lr(optimizer, new_lr):\n",
    "    # Run this to change the learning rate to 1e-5:\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b85c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    agents,\n",
    "    epochs=1000,\n",
    "    collection_steps_per_epoch=10000,\n",
    "    updates_per_epoch=500,\n",
    "    test_frequency=5,\n",
    "    test_steps=50000,\n",
    "    save_frequency = 50,\n",
    "    batch_size = 32,\n",
    "    eps = 0.1,\n",
    "    training_history = []\n",
    "):\n",
    "    for i in tqdm(range(epochs)):\n",
    "        \n",
    "        # Collection step\n",
    "        result = train_collector.collect(n_step = collection_steps_per_epoch)\n",
    "        \n",
    "        # Test Step\n",
    "        if i%test_frequency == 0:\n",
    "            for a in agents:\n",
    "                policy.policies[a].set_eps(0)\n",
    "            result = test_collector.collect(n_step = test_steps)\n",
    "            mean_reward = result['rews'].mean()\n",
    "            tqdm.write(str(mean_reward))\n",
    "            training_history.append(mean_reward)\n",
    "            for a in agents:\n",
    "                policy.policies[a].set_eps(eps)\n",
    "    \n",
    "        if i%save_frequency == 0:\n",
    "            save_policy(policy, agents)\n",
    "            save_history(training_history)\n",
    "    \n",
    "        # Update step (one epoch)\n",
    "        for _ in range(updates_per_epoch): \n",
    "            losses = policy.update(batch_size, train_collector.buffer)\n",
    "    \n",
    "    plot_and_save(training_history, test_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a19b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(training_history, test_frequency, save = True):\n",
    "    x = np.arange(len(training_history))\n",
    "    x *= test_frequency\n",
    "    plt.plot(x, training_history)\n",
    "    plt.title('Combined Average Score (Rainbow, 2 Color game)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Score (max 10)')\n",
    "    if save: plt.savefig(f'saved_data/training_group_2/training_curve.png')\n",
    "        \n",
    "def load(policy, path, agents):\n",
    "    for a in agents:\n",
    "        policy.policies[a].load_state_dict(torch.load(path + f'{a}_params.pth'))\n",
    "    his = list(np.load(path + f'training_rewards.npy'))\n",
    "    return his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf68087",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, agents, train_envs, test_envs = get_agents(lr = p['lr'], hidden_layers = p['hidden_layers'], gamma = p['gamma'], \n",
    "    target_update_freq = p['target_update_freq'], estimation_steps = p['estimation_steps'], num_train = p['num_train'], \n",
    "    num_test = p['num_test'])\n",
    "train_collector, test_collector = get_collectors(policy, train_envs, test_envs, buffer_size = p['buffer_size'])\n",
    "initialize_buffer(train_collector, p['buffer_size'], agents, policy, eps = p['eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f614d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = load(policy, path, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545f926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce9184d5dae45768724e463fe70c360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.054115507048659\n",
      "8.042476276547672\n",
      "7.999548532731377\n",
      "8.048670572329879\n",
      "8.079350766456267\n",
      "8.099188458070333\n",
      "8.124943617501128\n",
      "8.032897701667418\n",
      "8.055405405405406\n",
      "8.05507900677201\n",
      "8.083258325832583\n",
      "8.163292847503374\n",
      "8.128262826282628\n",
      "8.103386004514674\n",
      "8.064269662921348\n",
      "8.086642599277978\n",
      "8.131922557406574\n",
      "8.102390617952187\n",
      "8.05103884372177\n",
      "8.10521582733813\n",
      "8.074492099322798\n",
      "8.165693758419398\n",
      "8.11971197119712\n",
      "8.099412562132851\n",
      "8.113758992805755\n",
      "8.056884875846501\n",
      "8.091891891891892\n",
      "8.117303370786518\n",
      "8.085662759242561\n",
      "8.109353818346136\n",
      "8.10153429602888\n",
      "8.124044943820225\n",
      "8.163292847503374\n",
      "8.081227436823104\n",
      "8.105808194506979\n",
      "8.08228417266187\n",
      "8.097968397291197\n",
      "8.049616599007669\n",
      "8.046825754164791\n",
      "8.066335740072201\n",
      "8.115523465703971\n",
      "8.110661268556006\n",
      "8.10158013544018\n",
      "8.119208277103015\n",
      "8.128654970760234\n",
      "8.071139126519586\n",
      "8.074424898511502\n",
      "8.090418353576249\n",
      "8.074224021592443\n",
      "8.110207768744354\n",
      "8.128205128205128\n",
      "8.105168539325843\n",
      "8.115713642503376\n",
      "8.105026929982047\n",
      "8.147629796839729\n",
      "8.05668016194332\n",
      "8.078068592057761\n",
      "8.111211166141377\n",
      "8.099639314697926\n",
      "8.066726780883679\n",
      "8.143115124153498\n",
      "8.067056705670566\n",
      "8.157515751575158\n",
      "8.038409399005875\n",
      "8.082469580892294\n",
      "8.106450157870997\n",
      "8.145299145299145\n",
      "8.147297297297298\n",
      "8.107497741644083\n",
      "8.114620938628159\n",
      "8.101710171017102\n",
      "8.126178715761114\n",
      "8.09590274651058\n",
      "8.075607560756076\n",
      "8.119927862939585\n",
      "8.057761732851986\n",
      "8.061206120612061\n",
      "8.09613656783468\n",
      "8.140603875619648\n",
      "8.145421903052064\n",
      "8.143631436314363\n",
      "8.093285263632266\n",
      "8.12861010830325\n",
      "8.112910481331534\n",
      "8.089309878213802\n",
      "8.156123822341858\n",
      "8.15045045045045\n",
      "8.097791798107256\n",
      "8.171685393258427\n",
      "8.136998648039658\n",
      "8.100991884580703\n",
      "8.12275179856115\n",
      "8.106210621062106\n",
      "8.137122237257556\n",
      "8.134892086330936\n",
      "8.16005410279531\n",
      "8.042304230423042\n",
      "8.113309352517986\n",
      "8.080666967102298\n",
      "8.136302294197032\n",
      "8.125788999098287\n",
      "8.152946468735943\n",
      "8.115211521152116\n",
      "8.08258527827648\n",
      "8.090415913200724\n",
      "8.13515940727436\n",
      "8.075989208633093\n",
      "8.117408906882591\n",
      "8.121402877697841\n",
      "8.13204146011717\n",
      "8.15658844765343\n",
      "8.07370786516854\n",
      "8.127822944896115\n",
      "8.088407758231845\n",
      "8.087758775877587\n",
      "8.049661399548533\n",
      "8.082395317424584\n",
      "8.055405405405406\n",
      "8.121007647323436\n",
      "8.04408457040036\n",
      "8.129438202247192\n",
      "8.157207207207207\n",
      "8.136241007194245\n",
      "8.101717902350813\n",
      "8.150112359550562\n",
      "8.139377537212448\n",
      "8.128539325842697\n",
      "8.143114311431143\n"
     ]
    }
   ],
   "source": [
    "train(policy, train_collector, test_collector, agents,\n",
    "    epochs=p['epochs'],\n",
    "    collection_steps_per_epoch=p['steps_per_collect'],\n",
    "    updates_per_epoch=p['updates_per_train'],\n",
    "    test_frequency=p['test_frequency'],\n",
    "    test_steps=p['test_steps'],\n",
    "    save_frequency = p['save_frequency'],\n",
    "    batch_size = p['batch_size'],\n",
    "    eps = p['eps'],\n",
    "    training_history = training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73832f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c9945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a02f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
