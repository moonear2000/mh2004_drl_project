{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab612f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import getopt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import rl_env\n",
    "from myAgents.epsilon_greedy import GreedyAgent\n",
    "\n",
    "\"\"\"Some notes regarding this script:\n",
    "    1) define an epsilon greedy agent that starts exploring using random states and then explores more common states\n",
    "    2) define the tabular method for maintaining q values\n",
    "    3) define update rules for q-values\"\"\"\n",
    "\n",
    "# Changes to make:\n",
    "    # 1) Remove the 'reveal_color' option from the possible actions. This action is useless in this game\n",
    "    # 2) Change the reward structure so that +1 is acheived for each correct card played, and in terminal state +10 is given for 5 correctly played cards\n",
    "    # 3) Change the penalty structure so that -10 is given when 0 cards have been played and the game ends.\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"Runner class\"\"\"\n",
    "    def __init__(self, flags):\n",
    "    \"\"\"Initialize runner.\"\"\"\n",
    "    self.flags = flags\n",
    "    self.environment = rl_env.make('Hanabi-Very-Small', num_players=flags['players'])\n",
    "    self.agent_config = {'players': flags['players'], 'information_tokens': 3}\n",
    "    self.epsilon_initial = 0.9 # explores 'epsilon' of the time\n",
    "    self.agent_class = GreedyAgent\n",
    "    self.gamma = 1\n",
    "    self.lr_initial = 0.3 #fast learning\n",
    "    dirname = os.path.dirname(__file__)\n",
    "    #self.filename = os.path.join(dirname, 'tables/q_table_with_action1.pickle')\n",
    "    #self.logFileName = os.path.join(dirname, 'logs/q_table_with_action1_continued.pickle')\n",
    "    self.eval_interval = 100000\n",
    "    self.eval_iters = 10000\n",
    "    try:\n",
    "        with open(self.filename, 'rb') as fp:\n",
    "            self.q_table = pickle.load(fp)\n",
    "    except IOError:\n",
    "        print('No old table found, starting a new table')\n",
    "        self.q_table = {}\n",
    "\n",
    "    def learning_rate(self, iteration, total_iterations):\n",
    "    alpha = self.lr_initial * np.exp(-(iteration)*4/total_iterations)\n",
    "    if alpha < 0.1:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return alpha\n",
    "\n",
    "    def encode_state(self, observation, previous_action):\n",
    "    # The purpose of this function is to encode the state into a string that can be used as a key to the q_dict\n",
    "\n",
    "    state= ''\n",
    "    color = 'R'\n",
    "    # life tokens [0,1]\n",
    "    state += str(observation['life_tokens'])\n",
    "    # information tokens [0,1,2,3]\n",
    "    state += str(observation['information_tokens'])\n",
    "    # fireworks [0,1,2,3,4]\n",
    "    state += str(observation['fireworks'][color])\n",
    "    # discard pile: list of discarded numbers\n",
    "    discard_pile = ''\n",
    "    for card in observation['discard_pile']:\n",
    "        discard_pile += str(card['rank'])\n",
    "    state += 'X'*(10 - len(discard_pile)) + ''.join(sorted(discard_pile))\n",
    "\n",
    "    # current agent hand knowledge (must be sorted)\n",
    "    hand_knowledge = ''\n",
    "    for card in observation['card_knowledge'][0]:\n",
    "        if card['rank'] == None:\n",
    "            hand_knowledge += 'X'\n",
    "            continue\n",
    "        hand_knowledge += str(card['rank'])\n",
    "    if len(hand_knowledge)<2:\n",
    "        hand_knowledge += 'X'\n",
    "    state += hand_knowledge\n",
    "\n",
    "    # his hand (unsorted)\n",
    "    hand = ''\n",
    "    for card in observation['observed_hands'][-1]:\n",
    "        hand += str(card['rank'])\n",
    "    if len(hand) <2:\n",
    "        hand += 'X'\n",
    "    state += ''.join(sorted(hand))\n",
    "\n",
    "    # his hand knowledge (unsorted)\n",
    "    hand_knowledge = ''\n",
    "    for card in observation['card_knowledge'][1]:\n",
    "        if card['rank'] == None:\n",
    "            hand_knowledge += 'X'\n",
    "            continue\n",
    "        hand_knowledge += str(card['rank'])\n",
    "    if len(hand_knowledge) <2:\n",
    "        hand_knowledge += 'X'\n",
    "    state += ''.join(sorted(hand_knowledge))\n",
    "\n",
    "    state += previous_action\n",
    "    # the length of state string should be 19\n",
    "    if len(state) != 20:\n",
    "        print(observation)\n",
    "    assert len(state) == 20\n",
    "    return state\n",
    "\n",
    "    def find_q(self, state, action = None):\n",
    "\n",
    "    if action == None: # Find over all actions\n",
    "        max_value = 0\n",
    "        for a in self.q_table[state]['actions']:\n",
    "            max_value = max(a['value'], max_value)\n",
    "        return max_value\n",
    "    else:\n",
    "        for a in self.q_table[state]['actions']:\n",
    "            if a['action'] == action:\n",
    "                return a['value']\n",
    "        return 0\n",
    "\n",
    "    def update_q_table(self, old_state, new_state, action, reward, lr=0.1):\n",
    "\n",
    "    old_q = self.find_q(old_state, action)\n",
    "    new_q = self.find_q(new_state)\n",
    "    q = old_q + lr *(reward + self.gamma*(new_q) - old_q)\n",
    "\n",
    "    for a in self.q_table[old_state]['actions']:\n",
    "        if a['action'] == action:\n",
    "            a['value'] = q\n",
    "\n",
    "    def add_state_to_table(self, state, legal_moves):\n",
    "    self.q_table[state] = {'actions':[], 'e':1}\n",
    "    for a in legal_moves:\n",
    "        if type(a) == str:\n",
    "            self.q_table[state]['actions'].append({'action': a, 'value': 0})\n",
    "        elif 'COLOR' not in a['action_type']: # Do not add revealing color as a legal action (as it is irrelevant in this game)\n",
    "            self.q_table[state]['actions'].append({'action': a, 'value': 0})\n",
    "\n",
    "    def evaluate(self, num_games = 10000):\n",
    "\n",
    "    rewards = np.zeros(num_games)\n",
    "    unvisited_states = 0\n",
    "    for game in tqdm(range(num_games)):\n",
    "        observations = self.environment.reset()\n",
    "        agents = [self.agent_class(self.agent_config, 0) for _ in range(self.flags['players'])]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        reward = 0\n",
    "        previous_action = 'X'\n",
    "        while not done:\n",
    "\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                if observations['player_observations'][agent_id]['current_player_offset'] == 0:\n",
    "                    observation = observations['player_observations'][agent_id]\n",
    "                    state = self.encode_state(observation, previous_action)\n",
    "                    if state not in self.q_table:\n",
    "                        unvisited_states += 1\n",
    "                        action = random.choice(observation['legal_moves'])\n",
    "                    else:\n",
    "                        action = agent.act(state, self.q_table)\n",
    "                    action_index = observation['legal_moves'].index(action)\n",
    "                    previous_action = str(observation['legal_moves_as_int'][action_index])\n",
    "                    observations, reward, done, unused_info = self.environment.step(action)\n",
    "                    break\n",
    "\n",
    "            if reward == 1: episode_reward += reward\n",
    "\n",
    "        rewards[game] = episode_reward\n",
    "\n",
    "    average_score = np.mean(rewards)\n",
    "    print('Initial state value = {}'.format(self.q_table['X']))\n",
    "    print('The average score across {} games is {}'.format(num_games, average_score))\n",
    "    print('{} unvisited states'.format(unvisited_states))\n",
    "    r = {}\n",
    "    for i in rewards:\n",
    "        if i in r:\n",
    "            r[i] += 1\n",
    "        else:\n",
    "            r[i] = 1\n",
    "    return average_score, r\n",
    "\n",
    "    def play_games(self, num_games = 5):\n",
    "    \"Run episodes\"\n",
    "    rewards = []\n",
    "    for episode in range(num_games):\n",
    "        # Reset game\n",
    "        observations = self.environment.reset()\n",
    "        # Create agents\n",
    "        agents = [self.agent_class(self.agent_config, 0) for _ in range(self.flags['players'])]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        # q-parameters\n",
    "        old_state = 'X'\n",
    "        reward = 0\n",
    "        action = 'Start Game'\n",
    "        previous_action = 'X'\n",
    "\n",
    "        # Start game\n",
    "        print('Start Game')\n",
    "        while not done:\n",
    "            # At each turn, iterate through all agents\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "\n",
    "                # Only proceed for current agent\n",
    "                if observations['player_observations'][agent_id]['current_player_offset'] == 0:\n",
    "                    # Find that agents observations\n",
    "                    observation = observations['player_observations'][agent_id]\n",
    "                    print('Observation:')\n",
    "                    print(observation)\n",
    "                    print(\"***********\")\n",
    "                    new_state = self.encode_state(observation, previous_action)\n",
    "                    if new_state not in self.q_table:\n",
    "                        self.add_state_to_table(new_state, observation['legal_moves'])\n",
    "                    action = agent.act(new_state, self.q_table)\n",
    "                    for a in self.q_table[new_state]['actions']:\n",
    "                        if a['action'] == action:\n",
    "                            None\n",
    "                    assert action is not None\n",
    "                    print('Agent: {} action: {}'.format(observation['current_player'],\n",
    "                                            action))\n",
    "                    #print('Legal Moves: {}'.format(observation['legal_moves']))\n",
    "                    #print('His knowledge: {}'.format(observation['card_knowledge'][1]))\n",
    "                    action_index = observation['legal_moves'].index(action)\n",
    "                    previous_action = str(observation['legal_moves_as_int'][action_index])\n",
    "                    observations, reward, done, unused_info = self.environment.step(action)\n",
    "                    break\n",
    "\n",
    "            # Note when playing a correct card, reward is +1\n",
    "            if reward == 1: episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "        print('Episode Reward = {}'.format(episode_reward))\n",
    "\n",
    "  def explore_phase(self, num_iters = 2000000):\n",
    "    print('Exploration Phase for {} iterations'.format(num_iters))\n",
    "    log = []\n",
    "    for _ in tqdm(range(num_iters+1)):\n",
    "        # Reset game\n",
    "        lr = 0.15\n",
    "        observations = self.environment.reset()\n",
    "        # Create agents\n",
    "        agents = [self.agent_class(self.agent_config, 1) for _ in range(self.flags['players'])] # Agents are fully explorative\n",
    "        state_log = {0: [('X','Start Game')], 1:[('X','Start Game')]}\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        rewards = []\n",
    "        reward = 0\n",
    "        previous_action = 'X'\n",
    "\n",
    "        while not done:\n",
    "            rewards.append(reward)\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                if observations['player_observations'][agent_id]['current_player_offset'] == 0:\n",
    "                    observation = observations['player_observations'][agent_id]\n",
    "                    new_state = self.encode_state(observation, previous_action)\n",
    "                    if new_state not in self.q_table:\n",
    "                        self.add_state_to_table(new_state, observation['legal_moves'])\n",
    "                    self.update_q_table(state_log[agent_id][-1][0], new_state, state_log[agent_id][-1][1], sum(rewards[-2:]), lr=lr)\n",
    "                    action = agent.act(new_state, self.q_table)\n",
    "                    action_index = observation['legal_moves'].index(action)\n",
    "                    previous_action = str(observation['legal_moves_as_int'][action_index])\n",
    "                    state_log[agent_id].append((new_state, action))\n",
    "                    observations, reward, done, unused_info = self.environment.step(action)\n",
    "                    break\n",
    "            if reward == 1: episode_reward += reward\n",
    "        \n",
    "        # Adjust final states\n",
    "        if episode_reward == 0:\n",
    "            final_reward = -10\n",
    "        elif episode_reward == 5: \n",
    "            final_reward = 10\n",
    "        else:\n",
    "            final_reward = 0\n",
    "        \n",
    "        for agent_id, agent in enumerate(agents):\n",
    "            self.update_q_table(state_log[agent_id][-1][0], 'Terminal', state_log[agent_id][-1][1], final_reward, lr=lr)\n",
    "        \n",
    "        if _ % self.eval_interval == 0:\n",
    "            log.append(self.evaluate(self.eval_iters))\n",
    "            #self.save_logger(log)\n",
    "\n",
    "    print('Saving table')\n",
    "    try:\n",
    "        with open(self.filename, 'wb') as fp:\n",
    "            pickle.dump(self.q_table, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print('Could not save table')\n",
    "    return log\n",
    "\n",
    "  def save_logger(self, log):\n",
    "    log_dict = {'log':log, 'interval': self.eval_interval, 'iters': self.eval_iters}\n",
    "    try:\n",
    "        with open(self.logFileName, 'wb') as dp:\n",
    "            pickle.dump(log_dict, dp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print('Could not save loggs')\n",
    "        \n",
    "    \n",
    "\n",
    "  def train(self):\n",
    "\n",
    "    # Improvements to make to q-learning training:\n",
    "        # 1) The reward in the update step should be the cumulative reward between actionable states (shared reward)\n",
    "        # 2) When a game is lost and no correct plays have been made, update with a penelty of -100\n",
    "        # 3) When a game is lost after x correct plays, update terminal states with a reward of x\n",
    "        # 4) Epsilon should be determined by the number of times that state has been visited. \n",
    "        #    It should decay with 1/visits. When a state has been visited multiple times, take the greedy route\n",
    "    # The idea behind 1 is that if a player makes an action to facilitate another player playing a card, he should be rewarded\n",
    "    # The idea behind 2 is to discourage playing moves that may result in a loss (i.e playing cards you don't know). Instead, only play cards you know, otherwise discard\n",
    "    # The idea behind 3 is to add a terminal reward at terminal states that can backpropogate through the state tree. Rewards given during gameplay are guidance towards terminal states.\n",
    "    \n",
    "\n",
    "    if 'X' not in self.q_table:\n",
    "            self.add_state_to_table('X', ['Start Game'])\n",
    "    \n",
    "    if 'Terminal' not in self.q_table:\n",
    "            self.add_state_to_table('Terminal', ['Invalid'])\n",
    "\n",
    "    \n",
    "    # Initial fully exploration phase\n",
    "    log = self.explore_phase(num_iters=1)\n",
    "    # Evaluate\n",
    "    print('Evaluating')\n",
    "    self.evaluate(self.eval_iters)\n",
    "    num_episodes = self.flags['num_episodes']\n",
    "    epsilon_decay = np.exp(np.log(0.05)/num_episodes)\n",
    "    epsilon_decay_linear = 1/(0.9*num_episodes)\n",
    "    epsilon = 1\n",
    "    \n",
    "\n",
    "    for episode in tqdm(range(num_episodes+1)):\n",
    "        # Reset game\n",
    "        lr = 0.01\n",
    "        observations = self.environment.reset()\n",
    "\n",
    "        # Create agents\n",
    "        # TO-DO: Change so that epsilon is set based on the number of times a state has been visited\n",
    "        agents = [self.agent_class(self.agent_config, 0.01) for _ in range(self.flags['players'])]\n",
    "        state_log = {0: [('X','Start Game')], 1:[('X','Start Game')]}\n",
    "        previous_action = 'X'\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        rewards = []\n",
    "        reward = 0\n",
    "\n",
    "        while not done:\n",
    "            rewards.append(reward)\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                if observations['player_observations'][agent_id]['current_player_offset'] == 0:\n",
    "                    observation = observations['player_observations'][agent_id]\n",
    "                    new_state = self.encode_state(observation, previous_action)\n",
    "                    if new_state not in self.q_table:\n",
    "                        self.add_state_to_table(new_state, observation['legal_moves'])\n",
    "                    self.q_table[new_state]['e'] *= epsilon_decay\n",
    "                    self.update_q_table(state_log[agent_id][-1][0], new_state, state_log[agent_id][-1][1], sum(rewards[-2:]), lr=lr)\n",
    "                    action = agent.act(new_state, self.q_table)\n",
    "                    action_index = observation['legal_moves'].index(action)\n",
    "                    previous_action = str(observation['legal_moves_as_int'][action_index])\n",
    "                    state_log[agent_id].append((new_state, action))\n",
    "                    observations, reward, done, unused_info = self.environment.step(action)\n",
    "                    \n",
    "                    break\n",
    "            if reward == 1: episode_reward += reward\n",
    "        \n",
    "        # Adjust terminal states\n",
    "        if episode_reward == 0:\n",
    "            final_reward = -10\n",
    "        elif episode_reward == 5: \n",
    "            final_reward = 10\n",
    "        else:\n",
    "            final_reward = 0\n",
    "        \n",
    "        for agent_id, agent in enumerate(agents):\n",
    "            self.update_q_table(state_log[agent_id][-1][0], 'Terminal', state_log[agent_id][-1][1], final_reward, lr=lr)\n",
    "\n",
    "        epsilon *= epsilon_decay\n",
    "        #epsilon -= epsilon_decay_linear\n",
    "            \n",
    "        if episode%self.eval_interval == 0 and episode != 0:\n",
    "            print('Saving table')\n",
    "            log.append(self.evaluate(self.eval_iters))\n",
    "            #self.save_logger(log)\n",
    "            try:\n",
    "                with open(self.filename, 'wb') as fp:\n",
    "                    pickle.dump(self.q_table, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            except:\n",
    "                print('Could not save')\n",
    "    \n",
    "    x = np.linspace(0,len(log), len(log))*self.eval_interval\n",
    "    plt.plot(x, log)\n",
    "    plt.scatter(x,log)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Expected game score (Max 5)')\n",
    "    plt.title('q-learning using 20D vector')\n",
    "    plt.savefig('20d_vec_q_learning1_part2.png')\n",
    "    \n",
    "    def plot(self, log, show = True):\n",
    "        x = np.linspace(0,len(log), len(log))*self.eval_interval\n",
    "        plt.plot(x, log)\n",
    "        plt.scatter(x,log)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Expected game score (Max 5)')\n",
    "        plt.title('q-learning using 20D vector')\n",
    "        plt.savefig('20d_vec_q_learning1_part2.png')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  flags = {'players': 2, 'num_episodes': 10000000, 'agent_class': 'RandomAgent'}\n",
    "  options, arguments = getopt.getopt(sys.argv[1:], '',\n",
    "                                     ['players=',\n",
    "                                      'num_episodes=',\n",
    "                                      'agent_class='])\n",
    "  for flag, value in options:\n",
    "    flag = flag[2:]  # Strip leading --.\n",
    "    flags[flag] = type(flags[flag])(value)\n",
    "  runner = Trainer(flags)\n",
    "  numIters = 100000\n",
    "  average, rewards = runner.evaluate(numIters)\n",
    "  r_sum = average*numIters\n",
    "  y_axis = np.arange(len(rewards))[::-1]\n",
    "  y_labels = np.arange(1, 6)\n",
    "  frequency = [rewards[i] for i in y_labels]\n",
    "  plt.bar(y_axis, frequency, align='center', alpha = 0.8)\n",
    "  plt.xticks(y_axis, y_labels)\n",
    "  for r, f in zip(y_axis, frequency):\n",
    "    plt.annotate('{}%'.format(round(f*100/numIters,2)), (r,f), ha='center', va='bottom')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title('Bar chart showing scores from q_learning. Average reward = {}'.format(round(r_sum/numIters, 2)))\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
