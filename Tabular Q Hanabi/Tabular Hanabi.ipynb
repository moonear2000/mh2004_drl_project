{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4563fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts\n",
    "import numpy as np\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    MultiAgentPolicyManager\n",
    ")\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from pettingzoo.classic import hanabi_v4\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e9335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-4,\n",
    "    'num_train':32,\n",
    "    'num_test':32,\n",
    "    'test_steps': 20000,\n",
    "    'epochs':10**7,\n",
    "    'test_frequency':20000,\n",
    "    'eps':0.01\n",
    "}\n",
    "path = 'results/test_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c15f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define a new policy class that is compatible with tabular q learning\n",
    "\"\"\" \n",
    "Function to define: \n",
    "1) forward (takes in a Batch, output a Batch)\n",
    "2) learn (takes in a Batch, outputs a Dict)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Tabular_QL(BasePolicy):\n",
    "    \n",
    "    def __init__(self, discount_factor = 1, learning_rate = 1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Q_table = {}\n",
    "        self.eps = 0\n",
    "        \n",
    "    def set_eps(self, eps):\n",
    "        self.eps = eps\n",
    "        \n",
    "    def train(self, mode: bool):\n",
    "        self.training = mode\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def process_fn(self, batch, buffer, indicies):\n",
    "        pass\n",
    "    \n",
    "    def learn(self, batch):\n",
    "        # Takes in a batch and updates the Q table.\n",
    "        q = self(batch).logits\n",
    "        q = q[np.arange(len(q)), batch.act]\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a86f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(render_mode=None):\n",
    "    return PettingZooEnv(hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=4, max_information_tokens=5,\n",
    "max_life_tokens=2, observation_type=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4c883a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents(\n",
    "    lr = 1e-4, \n",
    "    hidden_layers = [256, 256], \n",
    "    gamma = 0.99,\n",
    "    target_update_freq = 200, \n",
    "    estimation_steps = 1, \n",
    "    num_train = 50, \n",
    "    num_test = 50,\n",
    "    atom_size = 51,\n",
    "    noisy_std = 0.1):\n",
    "    \n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(\n",
    "    env.observation_space, gym.spaces.Dict\n",
    "    ) else env.observation_space\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    agent = Tabular_QL()\n",
    "    agents = [agent, agent]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    agents = env.agents\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(num_train)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(num_test)])\n",
    "    return policy, agents, train_envs, test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955d6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collectors(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    test_envs,\n",
    "    buffer_size\n",
    "):\n",
    "    \n",
    "    # Get collectors\n",
    "    train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(buffer_size, len(train_envs), alpha = 0.6, beta = 0.4, weight_norm=True),\n",
    "    exploration_noise=True)\n",
    "    \n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    \n",
    "    return train_collector, test_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e5e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(\n",
    "    train_collector,\n",
    "    buffer_size,\n",
    "    agents,\n",
    "    policy,\n",
    "    eps = 0.1\n",
    "):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(1)\n",
    "    train_collector.collect(n_step = buffer_size)\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42efc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy(policy, agents):\n",
    "    for a in agents:\n",
    "        torch.save(policy.policies[a].state_dict(), f'saved_data/training_group_2/{a}_params.pth')\n",
    "\n",
    "def save_history(history):\n",
    "    np.save(f'saved_data/training_group_2/training_rewards.npy', np.array(history))\n",
    "    \n",
    "def change_lr(optimizer, new_lr):\n",
    "    # Run this to change the learning rate to 1e-5:\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1fd7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    agents,\n",
    "    epochs=1000,\n",
    "    collection_steps_per_epoch=10000,\n",
    "    updates_per_epoch=500,\n",
    "    test_frequency=5,\n",
    "    test_steps=50000,\n",
    "    save_frequency = 50,\n",
    "    batch_size = 32,\n",
    "    eps = 0.1,\n",
    "    training_history = []\n",
    "):\n",
    "    for i in tqdm(range(epochs)):\n",
    "        \n",
    "        # Collection step\n",
    "        result = train_collector.collect(n_step = collection_steps_per_epoch)\n",
    "        \n",
    "        # Test Step\n",
    "        if i%test_frequency == 0:\n",
    "            for a in agents:\n",
    "                policy.policies[a].set_eps(0)\n",
    "            result = test_collector.collect(n_step = test_steps)\n",
    "            mean_reward = result['rews'].mean()\n",
    "            tqdm.write(str(mean_reward))\n",
    "            training_history.append(mean_reward)\n",
    "            for a in agents:\n",
    "                policy.policies[a].set_eps(eps)\n",
    "    \n",
    "        #if i%save_frequency == 0:\n",
    "            #save_policy(policy, agents)\n",
    "            #save_history(training_history)\n",
    "    \n",
    "        # Update step (one epoch)\n",
    "        for _ in range(updates_per_epoch): \n",
    "            losses = policy.update(batch_size, train_collector.buffer)\n",
    "    \n",
    "    plot_and_save(training_history, test_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "241b993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, agents, train_envs, test_envs = get_agents()\n",
    "train_collector, test_collector = get_collectors(policy, train_envs, test_envs, buffer_size = p['buffer_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4bd6d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3946384/1535849741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3946384/98864539.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(policy, train_collector, test_collector, agents, epochs, collection_steps_per_epoch, updates_per_epoch, test_frequency, test_steps, save_frequency, batch_size, eps, training_history)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Collection step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection_steps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Test Step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/data/collector.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# faster than retain_grad version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                         \u001b[0;31m# self.data.obs will be used by agent to get result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/multiagent/mapolicy.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'obs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0mtmp_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             out = policy(\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'batch'"
     ]
    }
   ],
   "source": [
    "train(policy, train_collector, test_collector, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230bdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
