{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017a78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.classic import hanabi_v4\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    RainbowPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy,\n",
    ")\n",
    "from tianshou.utils.net.discrete import NoisyLinear\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856220c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of rainbow used in deepmind paper\n",
    "p = {\n",
    "    'hidden_layers': [256,256],\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-4,\n",
    "    'target_update_freq': 500,\n",
    "    'estimation_steps': 1,\n",
    "    'num_train':32,\n",
    "    'num_test':16,\n",
    "    'buffer_size':50000,\n",
    "    'vmax':25,\n",
    "    'vmin':-25,\n",
    "    'noisy_std':0.1,\n",
    "    'atom_size':51,\n",
    "    'minimum_replay_history':512,\n",
    "    'batch_size':32,\n",
    "    'steps_per_collect': 10016,\n",
    "    'updates_per_train': 1563,\n",
    "    'test_steps': 20000,\n",
    "    'epochs':5000,\n",
    "    'eps_decay_period': 100,\n",
    "    'test_frequency': 3,\n",
    "    'test_eps': 0,\n",
    "    'save_frequency': 25,\n",
    "    'eps_final':0.05,\n",
    "    'adam_eps': 3.125e-5,\n",
    "    'path': 'results/hanabi_small_256_size4/',\n",
    "    'lr_scheduler_factor': 0.1,\n",
    "    'lr_scheduler_patience': 20\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1b13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(render_mode=None):\n",
    "    return PettingZooEnv(hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=4, max_information_tokens=3,\n",
    "max_life_tokens=1, observation_type=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14b47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents(p):\n",
    "    \n",
    "    def noisy_linear(x, y):\n",
    "        return NoisyLinear(x, y, p['noisy_std'])\n",
    "    \n",
    "    # Return Policy, Agents, Envs\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(\n",
    "    env.observation_space, gym.spaces.Dict\n",
    "    ) else env.observation_space\n",
    "\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net = Net(\n",
    "            state_shape,\n",
    "            action_shape,\n",
    "            hidden_sizes=p['hidden_layers'],\n",
    "            device = device,\n",
    "            softmax = True,\n",
    "            num_atoms = p['atom_size'],\n",
    "            dueling_param = ({\n",
    "                'linear_layer': noisy_linear\n",
    "            }, {\n",
    "                'linear_layer': noisy_linear})\n",
    "    )\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr= p['lr'], eps=p['adam_eps'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode = 'max', factor = p['lr_scheduler_factor'],\n",
    "                                                              patience = p['lr_scheduler_patience'])\n",
    "\n",
    "    agent = RainbowPolicy(\n",
    "            net,\n",
    "            optim,\n",
    "            p['gamma'],\n",
    "            num_atoms = p['atom_size'],\n",
    "            v_min = p['vmin'],\n",
    "            v_max = p['vmax'],\n",
    "            estimation_step = p['estimation_steps'],\n",
    "            target_update_freq=p['target_update_freq']\n",
    "        ).to(device)\n",
    "\n",
    "    agents = [agent, agent]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    agents = env.agents\n",
    "\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(p['num_train'])])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(p['num_test'])])\n",
    "    \n",
    "    return policy, agents, train_envs, test_envs, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b396937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collectors(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    test_envs,\n",
    "    p\n",
    "):\n",
    "    \n",
    "    # Get collectors\n",
    "    train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    PrioritizedVectorReplayBuffer(p['buffer_size'], len(train_envs), alpha = 0.6, beta = 0.4, weight_norm=True),\n",
    "    exploration_noise=True)\n",
    "    \n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    \n",
    "    return train_collector, test_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer(\n",
    "    train_collector,\n",
    "    agents,\n",
    "    policy,\n",
    "    p\n",
    "):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(1)\n",
    "    train_collector.collect(n_step = p['minimum_replay_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13665eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy(policy, agents, p):\n",
    "    for a in agents:\n",
    "        torch.save(policy.policies[a].state_dict(), f'{p[\"path\"]}{a}_params.pth')\n",
    "\n",
    "def save_history(history, p):\n",
    "    np.save(f'{p[\"path\"]}training_rewards.npy', np.array(history))\n",
    "    \n",
    "def change_lr(optimizer, new_lr):\n",
    "    # Run this to change the learning rate to 1e-5:\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b85c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps(iteration, p):\n",
    "    if iteration > p['eps_decay_period']:\n",
    "        return p['eps_final']\n",
    "    else:\n",
    "        gradient = (1 - p['eps_final'])/p['eps_decay_period']\n",
    "        return 1 - gradient*iteration\n",
    "        \n",
    "def set_eps(policy, agents, new_eps):\n",
    "    for a in agents:\n",
    "        policy.policies[a].set_eps(new_eps)\n",
    "        \n",
    "def train(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    agents,\n",
    "    p,\n",
    "    lr_scheduler,\n",
    "    training_history = []\n",
    "):\n",
    "\n",
    "    for i in tqdm(range(p['epochs'])):\n",
    "        \n",
    "        eps = get_eps(i, p)\n",
    "        set_eps(policy, agents, eps)\n",
    "        \n",
    "        # Collection step\n",
    "        result = train_collector.collect(n_step = p['steps_per_collect'])\n",
    "        \n",
    "        # Test Step\n",
    "        if i%p['test_frequency'] == 0:\n",
    "            set_eps(policy, agents, p['test_eps'])\n",
    "            result = test_collector.collect(n_step = p['test_steps'])\n",
    "            mean_reward = result['rews'].mean()\n",
    "            tqdm.write(str(mean_reward))\n",
    "            training_history.append(mean_reward)\n",
    "            set_eps(policy, agents, eps)\n",
    "            lr_scheduler.step(mean_reward)\n",
    "    \n",
    "        if i%p['save_frequency'] == 0:\n",
    "            save_policy(policy, agents,p)\n",
    "            save_history(training_history,p)\n",
    "            plot_and_save(training_history, p['test_frequency'],p, show = False)\n",
    "    \n",
    "        # Update step (one epoch)\n",
    "        for _ in range(p['updates_per_train']): \n",
    "            losses = policy.update(p['batch_size'], train_collector.buffer)\n",
    "    \n",
    "    plot_and_save(training_history, test_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a19b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(training_history, test_frequency, p, save = True, show = True):\n",
    "    x = np.arange(len(training_history))\n",
    "    x *= test_frequency\n",
    "    plt.plot(x, training_history)\n",
    "    plt.title('Combined Average Score (Rainbow, 2 Color game)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Score (max 10)')\n",
    "    if save: plt.savefig(f'{p[\"path\"]}training_curve.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "def load(policy, agents, p):\n",
    "    for a in agents:\n",
    "        policy.policies[a].load_state_dict(torch.load(f'{p[\"path\"]}{a}_params.pth'))\n",
    "    his = list(np.load(f'{p[\"path\"]}training_rewards.npy'))\n",
    "    return his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf68087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n",
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy, agents, train_envs, test_envs, lr_scheduler = get_agents(p)\n",
    "train_collector, test_collector = get_collectors(policy, train_envs, test_envs, p)\n",
    "initialize_buffer(train_collector, agents, policy, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f614d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_history = load(policy, agents,p)\n",
    "training_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c545f926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a906420680e8409fbe7cdee4f6206d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0052251421546027355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfs/mh2004/anaconda3/lib/python3.9/site-packages/tianshou/data/batch.py:546: UserWarning: You are using tensors with different shape, fallback to dtype=object by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075297225891678\n",
      "1.0039473684210527\n",
      "0.9654289372599232\n",
      "0.8131720430107527\n",
      "0.9549668874172186\n",
      "0.9933155080213903\n",
      "1.0388235294117647\n",
      "1.0\n",
      "1.0301507537688441\n",
      "1.0990220048899755\n",
      "0.43478260869565216\n",
      "1.2762557077625571\n",
      "1.1406672678088368\n",
      "1.1822810590631365\n",
      "1.1086691086691087\n",
      "1.3317490494296578\n",
      "1.2881165919282511\n",
      "1.3915211970074812\n",
      "1.4639830508474576\n",
      "1.449531737773153\n",
      "1.6367816091954024\n",
      "1.5152471083070451\n",
      "1.6343963553530751\n",
      "1.5917782026768643\n",
      "1.478483606557377\n",
      "1.81524926686217\n",
      "1.8086763070077865\n",
      "1.8777777777777778\n",
      "2.053551912568306\n",
      "2.0633333333333335\n",
      "2.567231638418079\n",
      "3.085308056872038\n",
      "3.0565167243367934\n",
      "3.5160919540229885\n",
      "3.734265734265734\n",
      "3.70501138952164\n",
      "3.8293515358361776\n",
      "4.2711471610660485\n",
      "4.211072664359862\n",
      "4.4422632794457275\n",
      "4.254256526674234\n",
      "4.5064027939464495\n",
      "4.548165137614679\n",
      "4.697175141242938\n",
      "4.672453703703703\n",
      "4.714448236632537\n",
      "4.640486725663717\n",
      "4.776754890678942\n",
      "4.741122565864834\n",
      "4.750849377123443\n",
      "4.678251121076233\n",
      "4.80652418447694\n",
      "4.927272727272728\n",
      "4.787709497206704\n",
      "4.889140271493213\n",
      "4.7933701657458565\n",
      "4.700913242009133\n",
      "5.06157354618016\n",
      "5.093997734994337\n",
      "4.7686230248307\n",
      "4.610619469026549\n",
      "5.088535754824064\n",
      "5.071025930101466\n",
      "4.886261261261262\n",
      "5.121412803532009\n",
      "4.921436588103255\n",
      "5.270762229806598\n",
      "5.15963511972634\n",
      "5.03037120359955\n",
      "5.363841807909605\n",
      "5.078801331853496\n",
      "5.241379310344827\n",
      "4.944008958566629\n",
      "5.1803097345132745\n",
      "5.359179019384264\n",
      "5.354473386183465\n",
      "5.480270574971815\n",
      "5.4199328107502796\n",
      "5.232662192393736\n",
      "5.291434927697441\n",
      "5.457722660653889\n",
      "5.363333333333333\n",
      "5.418141592920354\n",
      "5.51864406779661\n",
      "5.338268792710706\n",
      "5.599775784753363\n",
      "5.351791530944626\n",
      "5.4972314507198226\n",
      "5.255580357142857\n",
      "5.584343991179713\n",
      "5.636363636363637\n",
      "5.598442714126808\n",
      "5.5388026607538805\n",
      "5.637458926615553\n",
      "5.309549945115258\n",
      "5.530657748049053\n",
      "5.761957730812013\n",
      "5.396946564885496\n",
      "5.811023622047244\n",
      "5.63344407530454\n",
      "5.713495575221239\n",
      "5.641386782231852\n",
      "5.412147505422993\n",
      "5.620614035087719\n",
      "5.67146017699115\n",
      "5.6677704194260485\n",
      "5.719376391982182\n",
      "5.852842809364549\n",
      "5.519083969465649\n",
      "5.703260869565217\n",
      "5.416122004357298\n",
      "5.897891231964484\n",
      "5.632497273718648\n",
      "5.707182320441989\n",
      "5.794672586015539\n",
      "5.774086378737541\n",
      "5.9206174200661525\n",
      "5.912124582869855\n",
      "5.757409440175631\n",
      "6.025386313465783\n",
      "5.704935622317596\n",
      "5.8042780748663105\n",
      "5.837569060773481\n",
      "5.92239467849224\n",
      "5.972436604189636\n",
      "5.735745614035087\n",
      "5.882613510520487\n",
      "5.775\n",
      "6.006688963210703\n",
      "5.915938864628821\n",
      "5.893171806167401\n",
      "5.73224043715847\n",
      "5.964835164835165\n",
      "5.913566739606127\n",
      "6.047671840354767\n",
      "5.888888888888889\n",
      "6.092105263157895\n",
      "5.670329670329671\n",
      "5.627831715210356\n",
      "5.850713501646542\n",
      "6.088691796008869\n",
      "6.014301430143014\n",
      "5.991189427312776\n",
      "5.911209766925638\n",
      "6.084615384615384\n",
      "5.74537540805223\n",
      "6.006593406593407\n",
      "6.04550499445061\n",
      "6.0208562019758505\n",
      "5.866228070175438\n",
      "5.734204793028322\n",
      "6.066225165562914\n",
      "5.914660831509847\n",
      "5.952850877192983\n",
      "5.960044395116538\n",
      "6.092105263157895\n",
      "6.049668874172186\n",
      "6.202447163515017\n",
      "5.780911062906725\n",
      "5.956236323851203\n",
      "6.064622124863089\n",
      "6.021881838074398\n",
      "5.957095709570957\n",
      "6.060706401766004\n",
      "5.915929203539823\n",
      "5.93859649122807\n",
      "5.924611973392461\n",
      "5.849450549450549\n",
      "5.76226826608506\n",
      "6.243362831858407\n",
      "6.089108910891089\n",
      "6.089108910891089\n",
      "6.045054945054945\n",
      "6.049234135667396\n",
      "5.968306010928962\n",
      "6.022075055187638\n",
      "6.151447661469933\n",
      "6.305216426193119\n",
      "6.116997792494481\n",
      "5.934711643090315\n",
      "6.09171270718232\n",
      "6.12035010940919\n",
      "5.881637168141593\n",
      "5.99009900990099\n",
      "6.186740331491713\n",
      "6.230684326710817\n",
      "5.980540540540541\n",
      "5.841530054644808\n",
      "6.181616832779623\n",
      "6.156840934371524\n",
      "5.889130434782609\n",
      "6.176274944567628\n",
      "6.018660812294182\n",
      "6.138084632516704\n",
      "6.164113785557987\n",
      "6.041712403951702\n",
      "6.1662971175166295\n",
      "6.106710671067106\n",
      "6.224175824175824\n",
      "6.321826280623608\n",
      "6.378197997775306\n",
      "6.378737541528239\n",
      "6.360710321864595\n",
      "6.386061946902655\n",
      "6.426829268292683\n",
      "6.308710033076075\n",
      "6.450828729281768\n",
      "6.522702104097453\n",
      "6.316777041942605\n",
      "6.47292817679558\n",
      "6.394444444444445\n",
      "6.417777777777777\n",
      "6.369179600886918\n",
      "6.460088691796009\n",
      "6.406387665198238\n",
      "6.417410714285714\n",
      "6.463414634146342\n",
      "6.448123620309051\n",
      "6.524390243902439\n",
      "6.426503340757239\n",
      "6.488938053097345\n",
      "6.491051454138702\n",
      "6.4728682170542635\n",
      "6.456953642384106\n",
      "6.322974472807991\n",
      "6.518847006651884\n",
      "6.504434589800444\n",
      "6.43078626799557\n",
      "6.450937155457552\n",
      "6.490586932447398\n",
      "6.339601769911504\n",
      "6.507198228128461\n",
      "6.498888888888889\n",
      "6.559116022099447\n",
      "6.56\n",
      "6.511111111111111\n",
      "6.574279379157428\n",
      "6.5842572062084255\n",
      "6.529346622369879\n",
      "6.495016611295681\n",
      "6.528239202657807\n",
      "6.518763796909492\n",
      "6.526024363233666\n",
      "6.481194690265487\n",
      "6.525669642857143\n",
      "6.571113561190739\n",
      "6.543550165380375\n",
      "6.444816053511706\n",
      "6.59\n",
      "6.578420467185762\n",
      "6.581111111111111\n",
      "6.469478357380688\n",
      "6.58121546961326\n",
      "6.523915461624027\n",
      "6.496674057649668\n",
      "6.558139534883721\n",
      "6.473273942093541\n",
      "6.521690767519466\n",
      "6.488448844884489\n",
      "6.615982241953385\n",
      "6.431868131868132\n",
      "6.5391400220507165\n",
      "6.576327433628318\n",
      "6.512195121951219\n",
      "6.603563474387528\n",
      "6.578420467185762\n",
      "6.635150166852058\n",
      "6.476347634763476\n",
      "6.496674057649668\n",
      "6.548172757475083\n",
      "6.501664816870144\n",
      "6.548780487804878\n",
      "6.527012127894157\n",
      "6.506651884700665\n",
      "6.51\n",
      "6.508869179600887\n",
      "6.6452328159645235\n",
      "6.620267260579064\n",
      "6.5676274944567625\n",
      "6.506077348066299\n",
      "6.484478935698448\n",
      "6.423973362930077\n",
      "6.59866220735786\n",
      "6.548565121412803\n",
      "6.598224195338513\n",
      "6.553333333333334\n",
      "6.412348401323043\n",
      "6.51\n",
      "6.565410199556541\n",
      "6.652654867256637\n",
      "6.523915461624027\n",
      "6.556541019955654\n",
      "6.595580110497237\n",
      "6.544950055493896\n",
      "6.580931263858093\n",
      "6.548172757475083\n",
      "6.531631520532741\n",
      "6.526666666666666\n",
      "6.633740288568258\n",
      "6.559246954595792\n",
      "6.659267480577136\n",
      "6.606904231625835\n",
      "6.653761061946903\n",
      "6.590909090909091\n",
      "6.652654867256637\n",
      "6.618625277161862\n",
      "6.569536423841059\n",
      "6.604004449388209\n",
      "6.625\n",
      "6.6755555555555555\n",
      "6.6240266963292544\n",
      "6.5696902654867255\n",
      "6.557649667405765\n",
      "6.524444444444445\n",
      "6.644518272425249\n",
      "6.693244739756367\n",
      "6.541345093715546\n",
      "6.455445544554456\n",
      "6.525909592061742\n",
      "6.620575221238938\n",
      "6.592674805771365\n",
      "6.573626373626373\n",
      "6.487236403995561\n",
      "6.585746102449889\n",
      "6.5706340378198\n",
      "6.6126526082130965\n",
      "6.563053097345133\n",
      "6.5823204419889505\n",
      "6.607734806629834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_140182/3111622494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_140182/599579738.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(policy, train_collector, test_collector, agents, p, lr_scheduler, training_history)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Update step (one epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'updates_per_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mplot_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/base.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/multiagent/mapolicy.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/modelfree/rainbow.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# so that NoisyLinear takes effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tianshou/policy/modelfree/c51.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# ref: https://github.com/Kaixhin/Rainbow/blob/master/agent.py L94-100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prio-buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(policy, train_collector, test_collector, agents, p, lr_scheduler, training_history = training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73832f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c9945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a02f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
